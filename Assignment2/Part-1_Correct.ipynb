{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T20:08:25.697788Z",
     "iopub.status.busy": "2025-03-12T20:08:25.697483Z",
     "iopub.status.idle": "2025-03-12T20:08:36.081599Z",
     "shell.execute_reply": "2025-03-12T20:08:36.080766Z",
     "shell.execute_reply.started": "2025-03-12T20:08:25.697767Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.7256\n",
      "Epoch 1000, Loss: 0.6864\n",
      "Epoch 2000, Loss: 0.5383\n",
      "Epoch 3000, Loss: 0.2103\n",
      "Epoch 4000, Loss: 0.1397\n",
      "Epoch 5000, Loss: 0.1117\n",
      "Epoch 6000, Loss: 0.0965\n",
      "Epoch 7000, Loss: 0.0869\n",
      "Epoch 8000, Loss: 0.0801\n",
      "Epoch 9000, Loss: 0.0751\n",
      "Epoch 10000, Loss: 0.0711\n",
      "Epoch 11000, Loss: 0.0679\n",
      "Epoch 12000, Loss: 0.0653\n",
      "Epoch 13000, Loss: 0.0630\n",
      "Epoch 14000, Loss: 0.0611\n",
      "Epoch 15000, Loss: 0.0594\n",
      "Epoch 16000, Loss: 0.0579\n",
      "Epoch 17000, Loss: 0.0566\n",
      "Epoch 18000, Loss: 0.0554\n",
      "Epoch 19000, Loss: 0.0543\n",
      "Epoch 20000, Loss: 0.0533\n",
      "Epoch 21000, Loss: 0.0524\n",
      "Epoch 22000, Loss: 0.0516\n",
      "Epoch 23000, Loss: 0.0508\n",
      "Epoch 24000, Loss: 0.0501\n",
      "Epoch 25000, Loss: 0.0495\n",
      "Epoch 26000, Loss: 0.0489\n",
      "Epoch 27000, Loss: 0.0483\n",
      "Epoch 28000, Loss: 0.0478\n",
      "Epoch 29000, Loss: 0.0473\n",
      "Epoch 30000, Loss: 0.0468\n",
      "Epoch 31000, Loss: 0.0464\n",
      "Epoch 32000, Loss: 0.0459\n",
      "Epoch 33000, Loss: 0.0455\n",
      "Epoch 34000, Loss: 0.0451\n",
      "Epoch 35000, Loss: 0.0448\n",
      "Epoch 36000, Loss: 0.0444\n",
      "Epoch 37000, Loss: 0.0441\n",
      "Epoch 38000, Loss: 0.0438\n",
      "Epoch 39000, Loss: 0.0435\n",
      "Epoch 40000, Loss: 0.0432\n",
      "Epoch 41000, Loss: 0.0429\n",
      "Epoch 42000, Loss: 0.0427\n",
      "Epoch 43000, Loss: 0.0424\n",
      "Epoch 44000, Loss: 0.0422\n",
      "Epoch 45000, Loss: 0.0419\n",
      "Epoch 46000, Loss: 0.0417\n",
      "Epoch 47000, Loss: 0.0415\n",
      "Epoch 48000, Loss: 0.0412\n",
      "Epoch 49000, Loss: 0.0410\n",
      "\n",
      "Training Accuracy: 99.38%\n",
      "Test Accuracy: 99.50%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load and prepare data\n",
    "data = pd.read_csv('/kaggle/input/xor.csv')\n",
    "data = data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Split into features and labels\n",
    "X = data[['X1', 'X2']].values\n",
    "y = data['label'].values.reshape(-1, 1)\n",
    "\n",
    "# Train-test split (80-20)\n",
    "split_idx = int(0.8 * len(data))\n",
    "X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "\n",
    "# Define the MLP class\n",
    "class MLP:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.5, l2_lambda=0.001):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.l2_lambda = l2_lambda\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        np.random.seed(42)  # Ensure reproducibility\n",
    "        self.weights_input_hidden = np.random.randn(input_size, hidden_size) * np.sqrt(2. / input_size)\n",
    "        self.weights_hidden_output = np.random.randn(hidden_size, output_size) * np.sqrt(2. / hidden_size)\n",
    "        self.bias_hidden = np.zeros((1, hidden_size))\n",
    "        self.bias_output = np.zeros((1, output_size))\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-np.clip(x, -50, 50)))\n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # Forward pass\n",
    "        self.hidden_input = np.dot(X, self.weights_input_hidden) + self.bias_hidden\n",
    "        self.hidden_output = self.sigmoid(self.hidden_input)\n",
    "        \n",
    "        self.output_input = np.dot(self.hidden_output, self.weights_hidden_output) + self.bias_output\n",
    "        self.predicted_output = self.sigmoid(self.output_input)\n",
    "        return self.predicted_output\n",
    "    \n",
    "    def backward(self, X, y):\n",
    "        # Backward pass\n",
    "        m = len(X)  # Number of samples\n",
    "        \n",
    "        # Output layer error\n",
    "        d_error = (self.predicted_output - y) / m\n",
    "        \n",
    "        # Gradients for output layer\n",
    "        d_weights_hidden_output = np.dot(self.hidden_output.T, d_error) + \\\n",
    "                                  self.l2_lambda * self.weights_hidden_output / m\n",
    "        d_bias_output = np.sum(d_error, axis=0, keepdims=True)\n",
    "        \n",
    "        # Hidden layer error\n",
    "        d_hidden = np.dot(d_error, self.weights_hidden_output.T) * self.sigmoid_derivative(self.hidden_output)\n",
    "        \n",
    "        # Gradients for hidden layer\n",
    "        d_weights_input_hidden = np.dot(X.T, d_hidden) + \\\n",
    "                                 self.l2_lambda * self.weights_input_hidden / m\n",
    "        d_bias_hidden = np.sum(d_hidden, axis=0, keepdims=True)\n",
    "        \n",
    "        return d_weights_input_hidden, d_bias_hidden, d_weights_hidden_output, d_bias_output\n",
    "    \n",
    "    def update_parameters(self, d_weights_input_hidden, d_bias_hidden, d_weights_hidden_output, d_bias_output):\n",
    "        # Update weights and biases\n",
    "        self.weights_input_hidden -= self.learning_rate * d_weights_input_hidden\n",
    "        self.bias_hidden -= self.learning_rate * d_bias_hidden\n",
    "        self.weights_hidden_output -= self.learning_rate * d_weights_hidden_output\n",
    "        self.bias_output -= self.learning_rate * d_bias_output\n",
    "    \n",
    "    def compute_loss(self, y_true):\n",
    "        # Binary cross-entropy loss with L2 regularization\n",
    "        epsilon = 1e-8\n",
    "        data_loss = -np.mean(y_true * np.log(self.predicted_output + epsilon) + \\\n",
    "                    (1 - y_true) * np.log(1 - self.predicted_output + epsilon))\n",
    "        reg_loss = 0.5 * self.l2_lambda * (np.sum(self.weights_input_hidden**2) + \\\n",
    "                   np.sum(self.weights_hidden_output**2)) / len(y_true)\n",
    "        total_loss = data_loss + reg_loss\n",
    "        return total_loss\n",
    "    \n",
    "    def evaluate(self, X, y_true):\n",
    "        # Evaluate accuracy\n",
    "        predictions = self.forward(X)\n",
    "        predictions = (predictions > 0.5).astype(int)\n",
    "        accuracy = np.mean(predictions == y_true)\n",
    "        return accuracy\n",
    "\n",
    "# Initialize MLP\n",
    "mlp = MLP(input_size=2, hidden_size=4, output_size=1, learning_rate=0.5, l2_lambda=0.001)\n",
    "\n",
    "# Training loop with early stopping\n",
    "best_loss = np.inf\n",
    "patience = 500\n",
    "no_improvement = 0\n",
    "epochs = 50000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    mlp.forward(X_train)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = mlp.compute_loss(y_train)\n",
    "    \n",
    "    # Early stopping check\n",
    "    if loss < best_loss:\n",
    "        best_loss = loss\n",
    "        no_improvement = 0\n",
    "    else:\n",
    "        no_improvement += 1\n",
    "        \n",
    "    if no_improvement >= patience:\n",
    "        print(f\"Early stopping at epoch {epoch}\")\n",
    "        break\n",
    "    \n",
    "    # Backward pass\n",
    "    d_weights_input_hidden, d_bias_hidden, d_weights_hidden_output, d_bias_output = mlp.backward(X_train, y_train)\n",
    "    \n",
    "    # Update parameters\n",
    "    mlp.update_parameters(d_weights_input_hidden, d_bias_hidden, d_weights_hidden_output, d_bias_output)\n",
    "    \n",
    "    if epoch % 1000 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss:.4f}')\n",
    "\n",
    "# Evaluate on training and test sets\n",
    "train_accuracy = mlp.evaluate(X_train, y_train)\n",
    "test_accuracy = mlp.evaluate(X_test, y_test)\n",
    "\n",
    "print(f\"\\nTraining Accuracy: {train_accuracy * 100:.2f}%\")\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 3587583,
     "sourceId": 6243656,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
