{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6243656,"sourceType":"datasetVersion","datasetId":3587583}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T16:46:02.720136Z","iopub.execute_input":"2025-03-11T16:46:02.720530Z","iopub.status.idle":"2025-03-11T16:46:02.730937Z","shell.execute_reply.started":"2025-03-11T16:46:02.720497Z","shell.execute_reply":"2025-03-11T16:46:02.729901Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/xor.csv\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\n# Load and prepare data\ndata = pd.read_csv('/kaggle/input/xor.csv')\ndata = data.sample(frac=1, random_state=42).reset_index(drop=True)\n\n# Split into features and labels\nX = data[['X1', 'X2']].values\ny = data['label'].values.reshape(-1, 1)\n\n# Train-test split (80-20)\nsplit_idx = int(0.8 * len(data))\nX_train, X_test = X[:split_idx], X[split_idx:]\ny_train, y_test = y[:split_idx], y[split_idx:]\n\n# Enhanced activation functions with numerical stability\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-np.clip(x, -50, 50)))\n\ndef sigmoid_derivative(x):\n    return x * (1 - x)\n\n# Optimized network parameters\ninput_size = 2\nhidden_size = 4  # Reduced from 16 to 4 for XOR complexity match\noutput_size = 1\nlearning_rate = 0.5  # Restored original learning rate\nepochs = 50000\nepsilon = 1e-8\nl2_lambda = 0.001  # L2 regularization\n\n# Improved initialization\nnp.random.seed(42)\nweights_input_hidden = np.random.randn(input_size, hidden_size) * np.sqrt(2. / input_size)\nweights_hidden_output = np.random.randn(hidden_size, output_size) * np.sqrt(2. / hidden_size)\nbias_hidden = np.zeros((1, hidden_size))\nbias_output = np.zeros((1, output_size))\n\n# Training loop with early stopping\nbest_loss = np.inf\npatience = 500\nno_improvement = 0\n\nfor epoch in range(epochs):\n    # Forward propagation\n    hidden_input = np.dot(X_train, weights_input_hidden) + bias_hidden\n    hidden_output = sigmoid(hidden_input)\n    \n    output_input = np.dot(hidden_output, weights_hidden_output) + bias_output\n    predicted_output = sigmoid(output_input)\n    \n    # Regularized loss calculation\n    data_loss = -np.mean(y_train * np.log(predicted_output + epsilon) + \n                (1 - y_train) * np.log(1 - predicted_output + epsilon))\n    reg_loss = 0.5 * l2_lambda * (np.sum(weights_input_hidden**2) + \n               np.sum(weights_hidden_output**2)) / len(X_train)\n    total_loss = data_loss + reg_loss\n    \n    # Early stopping check\n    if total_loss < best_loss:\n        best_loss = total_loss\n        no_improvement = 0\n    else:\n        no_improvement += 1\n        \n    if no_improvement >= patience:\n        print(f\"Early stopping at epoch {epoch}\")\n        break\n    \n    # Backpropagation with regularization\n    d_error = (predicted_output - y_train) / len(X_train)\n    \n    # Output layer gradients\n    d_weights_hidden_output = np.dot(hidden_output.T, d_error) + l2_lambda * weights_hidden_output / len(X_train)\n    d_bias_output = np.sum(d_error, axis=0, keepdims=True)\n    \n    # Hidden layer gradients\n    d_hidden = np.dot(d_error, weights_hidden_output.T) * sigmoid_derivative(hidden_output)\n    d_weights_input_hidden = np.dot(X_train.T, d_hidden) + l2_lambda * weights_input_hidden / len(X_train)\n    d_bias_hidden = np.sum(d_hidden, axis=0, keepdims=True)\n    \n    # Update parameters\n    weights_hidden_output -= learning_rate * d_weights_hidden_output\n    bias_output -= learning_rate * d_bias_output\n    weights_input_hidden -= learning_rate * d_weights_input_hidden\n    bias_hidden -= learning_rate * d_bias_hidden\n\n    if epoch % 1000 == 0:\n        print(f'Epoch {epoch}, Loss: {total_loss:.4f}')\n\n# Evaluation function\ndef evaluate(X, y_true):\n    hidden_input = np.dot(X, weights_input_hidden) + bias_hidden\n    hidden_output = sigmoid(hidden_input)\n    output_input = np.dot(hidden_output, weights_hidden_output) + bias_output\n    predictions = (sigmoid(output_input) > 0.5).astype(int)\n    accuracy = np.mean(predictions == y_true)\n    return accuracy\n\n# Calculate metrics\ntrain_accuracy = evaluate(X_train, y_train)\ntest_accuracy = evaluate(X_test, y_test)\n\nprint(f\"\\nTraining Accuracy: {train_accuracy * 100:.2f}%\")\nprint(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T19:53:16.035662Z","iopub.execute_input":"2025-03-12T19:53:16.035949Z","iopub.status.idle":"2025-03-12T19:53:26.752006Z","shell.execute_reply.started":"2025-03-12T19:53:16.035926Z","shell.execute_reply":"2025-03-12T19:53:26.751101Z"}},"outputs":[{"name":"stdout","text":"Epoch 0, Loss: 0.7256\nEpoch 1000, Loss: 0.6864\nEpoch 2000, Loss: 0.5383\nEpoch 3000, Loss: 0.2103\nEpoch 4000, Loss: 0.1397\nEpoch 5000, Loss: 0.1117\nEpoch 6000, Loss: 0.0965\nEpoch 7000, Loss: 0.0869\nEpoch 8000, Loss: 0.0801\nEpoch 9000, Loss: 0.0751\nEpoch 10000, Loss: 0.0711\nEpoch 11000, Loss: 0.0679\nEpoch 12000, Loss: 0.0653\nEpoch 13000, Loss: 0.0630\nEpoch 14000, Loss: 0.0611\nEpoch 15000, Loss: 0.0594\nEpoch 16000, Loss: 0.0579\nEpoch 17000, Loss: 0.0566\nEpoch 18000, Loss: 0.0554\nEpoch 19000, Loss: 0.0543\nEpoch 20000, Loss: 0.0533\nEpoch 21000, Loss: 0.0524\nEpoch 22000, Loss: 0.0516\nEpoch 23000, Loss: 0.0508\nEpoch 24000, Loss: 0.0501\nEpoch 25000, Loss: 0.0495\nEpoch 26000, Loss: 0.0489\nEpoch 27000, Loss: 0.0483\nEpoch 28000, Loss: 0.0478\nEpoch 29000, Loss: 0.0473\nEpoch 30000, Loss: 0.0468\nEpoch 31000, Loss: 0.0464\nEpoch 32000, Loss: 0.0459\nEpoch 33000, Loss: 0.0455\nEpoch 34000, Loss: 0.0451\nEpoch 35000, Loss: 0.0448\nEpoch 36000, Loss: 0.0444\nEpoch 37000, Loss: 0.0441\nEpoch 38000, Loss: 0.0438\nEpoch 39000, Loss: 0.0435\nEpoch 40000, Loss: 0.0432\nEpoch 41000, Loss: 0.0429\nEpoch 42000, Loss: 0.0427\nEpoch 43000, Loss: 0.0424\nEpoch 44000, Loss: 0.0422\nEpoch 45000, Loss: 0.0419\nEpoch 46000, Loss: 0.0417\nEpoch 47000, Loss: 0.0415\nEpoch 48000, Loss: 0.0412\nEpoch 49000, Loss: 0.0410\n\nTraining Accuracy: 99.38%\nTest Accuracy: 99.50%\n","output_type":"stream"}],"execution_count":32}]}