{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-13T18:51:32.048734Z",
     "iopub.status.busy": "2025-03-13T18:51:32.048358Z",
     "iopub.status.idle": "2025-03-13T18:51:32.069176Z",
     "shell.execute_reply": "2025-03-13T18:51:32.068056Z",
     "shell.execute_reply.started": "2025-03-13T18:51:32.048705Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/kaggle/input/social-media-sentiments-analysis-dataset/sentimentdataset.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')  # Optional: Improves WordNet performance\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T18:51:32.070975Z",
     "iopub.status.busy": "2025-03-13T18:51:32.070640Z",
     "iopub.status.idle": "2025-03-13T18:51:32.086307Z",
     "shell.execute_reply": "2025-03-13T18:51:32.085078Z",
     "shell.execute_reply.started": "2025-03-13T18:51:32.070918Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv(\"/kaggle/input/social-media-sentiments-analysis-dataset/sentimentdataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T18:51:32.089479Z",
     "iopub.status.busy": "2025-03-13T18:51:32.089153Z",
     "iopub.status.idle": "2025-03-13T18:51:32.195492Z",
     "shell.execute_reply": "2025-03-13T18:51:32.194410Z",
     "shell.execute_reply.started": "2025-03-13T18:51:32.089452Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /kaggle/working/...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /kaggle/working/...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "Sentiment\n",
      "positive                45\n",
      "joy                     44\n",
      "excitement              37\n",
      "contentment             19\n",
      "neutral                 18\n",
      "gratitude               18\n",
      "curiosity               16\n",
      "serenity                15\n",
      "happy                   14\n",
      "nostalgia               11\n",
      "despair                 11\n",
      "grief                    9\n",
      "awe                      9\n",
      "sad                      9\n",
      "hopeful                  9\n",
      "loneliness               9\n",
      "embarrassed              8\n",
      "acceptance               8\n",
      "confusion                8\n",
      "euphoria                 7\n",
      "elation                  7\n",
      "enthusiasm               7\n",
      "pride                    7\n",
      "determination            7\n",
      "regret                   6\n",
      "frustration              6\n",
      "ambivalence              6\n",
      "melancholy               6\n",
      "numbness                 6\n",
      "playful                  6\n",
      "indifference             6\n",
      "bad                      6\n",
      "hate                     6\n",
      "surprise                 6\n",
      "inspiration              6\n",
      "bitterness               5\n",
      "frustrated               5\n",
      "betrayal                 5\n",
      "hope                     5\n",
      "happiness                5\n",
      "disgust                  5\n",
      "inspired                 5\n",
      "empowerment              5\n",
      "proud                    4\n",
      "grateful                 4\n",
      "thrill                   4\n",
      "overwhelmed              4\n",
      "compassionate            4\n",
      "reflection               4\n",
      "enchantment              4\n",
      "desolation               4\n",
      "negative                 4\n",
      "admiration               4\n",
      "boredom                  4\n",
      "calmness                 4\n",
      "reverence                4\n",
      "fulfillment              4\n",
      "compassion               4\n",
      "arousal                  4\n",
      "tenderness               4\n",
      "amusement                3\n",
      "anticipation             3\n",
      "envious                  3\n",
      "dismissive               3\n",
      "bitter                   3\n",
      "heartbreak               3\n",
      "adventure                3\n",
      "devastated               3\n",
      "satisfaction             3\n",
      "wonder                   3\n",
      "accomplishment           3\n",
      "creativity               3\n",
      "harmony                  3\n",
      "kind                     3\n",
      "jealous                  3\n",
      "love                     3\n",
      "fearful                  3\n",
      "confident                3\n",
      "free-spirited            3\n",
      "resentment               3\n",
      "empathetic               3\n",
      "shame                    3\n",
      "jealousy                 3\n",
      "sorrow                   2\n",
      "exploration              2\n",
      "captivation              2\n",
      "tranquility              2\n",
      "radiance                 2\n",
      "loss                     2\n",
      "mischievous              2\n",
      "rejuvenation             2\n",
      "resilience               2\n",
      "emotion                  2\n",
      "disappointment           2\n",
      "isolation                2\n",
      "coziness                 2\n",
      "whimsy                   2\n",
      "intimidation             2\n",
      "contemplation            2\n",
      "anxiety                  2\n",
      "helplessness             2\n",
      "envy                     2\n",
      "anger                    2\n",
      "zest                     2\n",
      "yearning                 2\n",
      "apprehensive             2\n",
      "fear                     2\n",
      "sadness                  2\n",
      "enjoyment                2\n",
      "adoration                2\n",
      "affection                2\n",
      "disappointed             2\n",
      "engagement               1\n",
      "obstacle                 1\n",
      "heartwarming             1\n",
      "triumph                  1\n",
      "suspense                 1\n",
      "touched                  1\n",
      "runway creativity        1\n",
      "sympathy                 1\n",
      "iconic                   1\n",
      "connection               1\n",
      "hypnotic                 1\n",
      "colorful                 1\n",
      "ecstasy                  1\n",
      "charm                    1\n",
      "journey                  1\n",
      "pressure                 1\n",
      "ocean's freedom          1\n",
      "relief                   1\n",
      "creative inspiration     1\n",
      "celestial wonder         1\n",
      "nature's beauty          1\n",
      "thrilling journey        1\n",
      "winter magic             1\n",
      "culinary adventure       1\n",
      "mesmerizing              1\n",
      "vibrancy                 1\n",
      "imagination              1\n",
      "envisioning history      1\n",
      "joy in baking            1\n",
      "breakthrough             1\n",
      "solace                   1\n",
      "celebration              1\n",
      "miscalculation           1\n",
      "renewed effort           1\n",
      "whispers of the past     1\n",
      "challenge                1\n",
      "mindfulness              1\n",
      "energy                   1\n",
      "melodic                  1\n",
      "motivation               1\n",
      "culinaryodyssey          1\n",
      "artisticburst            1\n",
      "adrenaline               1\n",
      "dazzle                   1\n",
      "freedom                  1\n",
      "innerjourney             1\n",
      "festivejoy               1\n",
      "joyfulreunion            1\n",
      "grandeur                 1\n",
      "blessed                  1\n",
      "appreciation             1\n",
      "confidence               1\n",
      "wonderment               1\n",
      "optimism                 1\n",
      "pensive                  1\n",
      "playfuljoy               1\n",
      "elegance                 1\n",
      "immersion                1\n",
      "spark                    1\n",
      "marvel                   1\n",
      "overjoyed                1\n",
      "dreamchaser              1\n",
      "romance                  1\n",
      "amazement                1\n",
      "success                  1\n",
      "friendship               1\n",
      "kindness                 1\n",
      "positivity               1\n",
      "solitude                 1\n",
      "heartache                1\n",
      "ruin                     1\n",
      "desperation              1\n",
      "darkness                 1\n",
      "exhaustion               1\n",
      "lostlove                 1\n",
      "emotionalstorm           1\n",
      "suffering                1\n",
      "bittersweet              1\n",
      "intrigue                 1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import subprocess\n",
    "\n",
    "nltk.download('wordnet', download_dir='/kaggle/working/')\n",
    "nltk.download('omw-1.4', download_dir='/kaggle/working/')\n",
    "\n",
    "command = \"unzip /kaggle/working/corpora/wordnet.zip -d /kaggle/working/corpora\"\n",
    "subprocess.run(command.split())\n",
    "\n",
    "nltk.data.path.append('/kaggle/working/')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "train_df['Sentiment'] = train_df['Sentiment'].astype(str).str.strip().str.lower().apply(lemmatizer.lemmatize)\n",
    "print(train_df['Sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T18:51:32.197408Z",
     "iopub.status.busy": "2025-03-13T18:51:32.197025Z",
     "iopub.status.idle": "2025-03-13T18:51:32.209312Z",
     "shell.execute_reply": "2025-03-13T18:51:32.207741Z",
     "shell.execute_reply.started": "2025-03-13T18:51:32.197366Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acceptance',\n",
       " 'accomplishment',\n",
       " 'admiration',\n",
       " 'adoration',\n",
       " 'adrenaline',\n",
       " 'adventure',\n",
       " 'affection',\n",
       " 'amazement',\n",
       " 'ambivalence',\n",
       " 'amusement',\n",
       " 'anger',\n",
       " 'anticipation',\n",
       " 'anxiety',\n",
       " 'appreciation',\n",
       " 'apprehensive',\n",
       " 'arousal',\n",
       " 'artisticburst',\n",
       " 'awe',\n",
       " 'bad',\n",
       " 'betrayal',\n",
       " 'bitter',\n",
       " 'bitterness',\n",
       " 'bittersweet',\n",
       " 'blessed',\n",
       " 'boredom',\n",
       " 'breakthrough',\n",
       " 'calmness',\n",
       " 'captivation',\n",
       " 'celebration',\n",
       " 'celestial wonder',\n",
       " 'challenge',\n",
       " 'charm',\n",
       " 'colorful',\n",
       " 'compassion',\n",
       " 'compassionate',\n",
       " 'confidence',\n",
       " 'confident',\n",
       " 'confusion',\n",
       " 'connection',\n",
       " 'contemplation',\n",
       " 'contentment',\n",
       " 'coziness',\n",
       " 'creative inspiration',\n",
       " 'creativity',\n",
       " 'culinary adventure',\n",
       " 'culinaryodyssey',\n",
       " 'curiosity',\n",
       " 'darkness',\n",
       " 'dazzle',\n",
       " 'desolation',\n",
       " 'despair',\n",
       " 'desperation',\n",
       " 'determination',\n",
       " 'devastated',\n",
       " 'disappointed',\n",
       " 'disappointment',\n",
       " 'disgust',\n",
       " 'dismissive',\n",
       " 'dreamchaser',\n",
       " 'ecstasy',\n",
       " 'elation',\n",
       " 'elegance',\n",
       " 'embarrassed',\n",
       " 'emotion',\n",
       " 'emotionalstorm',\n",
       " 'empathetic',\n",
       " 'empowerment',\n",
       " 'enchantment',\n",
       " 'energy',\n",
       " 'engagement',\n",
       " 'enjoyment',\n",
       " 'enthusiasm',\n",
       " 'envious',\n",
       " 'envisioning history',\n",
       " 'envy',\n",
       " 'euphoria',\n",
       " 'excitement',\n",
       " 'exhaustion',\n",
       " 'exploration',\n",
       " 'fear',\n",
       " 'fearful',\n",
       " 'festivejoy',\n",
       " 'free-spirited',\n",
       " 'freedom',\n",
       " 'friendship',\n",
       " 'frustrated',\n",
       " 'frustration',\n",
       " 'fulfillment',\n",
       " 'grandeur',\n",
       " 'grateful',\n",
       " 'gratitude',\n",
       " 'grief',\n",
       " 'happiness',\n",
       " 'happy',\n",
       " 'harmony',\n",
       " 'hate',\n",
       " 'heartache',\n",
       " 'heartbreak',\n",
       " 'heartwarming',\n",
       " 'helplessness',\n",
       " 'hope',\n",
       " 'hopeful',\n",
       " 'hypnotic',\n",
       " 'iconic',\n",
       " 'imagination',\n",
       " 'immersion',\n",
       " 'indifference',\n",
       " 'innerjourney',\n",
       " 'inspiration',\n",
       " 'inspired',\n",
       " 'intimidation',\n",
       " 'intrigue',\n",
       " 'isolation',\n",
       " 'jealous',\n",
       " 'jealousy',\n",
       " 'journey',\n",
       " 'joy',\n",
       " 'joy in baking',\n",
       " 'joyfulreunion',\n",
       " 'kind',\n",
       " 'kindness',\n",
       " 'loneliness',\n",
       " 'loss',\n",
       " 'lostlove',\n",
       " 'love',\n",
       " 'marvel',\n",
       " 'melancholy',\n",
       " 'melodic',\n",
       " 'mesmerizing',\n",
       " 'mindfulness',\n",
       " 'miscalculation',\n",
       " 'mischievous',\n",
       " 'motivation',\n",
       " \"nature's beauty\",\n",
       " 'negative',\n",
       " 'neutral',\n",
       " 'nostalgia',\n",
       " 'numbness',\n",
       " 'obstacle',\n",
       " \"ocean's freedom\",\n",
       " 'optimism',\n",
       " 'overjoyed',\n",
       " 'overwhelmed',\n",
       " 'pensive',\n",
       " 'playful',\n",
       " 'playfuljoy',\n",
       " 'positive',\n",
       " 'positivity',\n",
       " 'pressure',\n",
       " 'pride',\n",
       " 'proud',\n",
       " 'radiance',\n",
       " 'reflection',\n",
       " 'regret',\n",
       " 'rejuvenation',\n",
       " 'relief',\n",
       " 'renewed effort',\n",
       " 'resentment',\n",
       " 'resilience',\n",
       " 'reverence',\n",
       " 'romance',\n",
       " 'ruin',\n",
       " 'runway creativity',\n",
       " 'sad',\n",
       " 'sadness',\n",
       " 'satisfaction',\n",
       " 'serenity',\n",
       " 'shame',\n",
       " 'solace',\n",
       " 'solitude',\n",
       " 'sorrow',\n",
       " 'spark',\n",
       " 'success',\n",
       " 'suffering',\n",
       " 'surprise',\n",
       " 'suspense',\n",
       " 'sympathy',\n",
       " 'tenderness',\n",
       " 'thrill',\n",
       " 'thrilling journey',\n",
       " 'touched',\n",
       " 'tranquility',\n",
       " 'triumph',\n",
       " 'vibrancy',\n",
       " 'whimsy',\n",
       " 'whispers of the past',\n",
       " 'winter magic',\n",
       " 'wonder',\n",
       " 'wonderment',\n",
       " 'yearning',\n",
       " 'zest'}"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_values = set()\n",
    "unique_values.update(train_df['Sentiment'])\n",
    "unique_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T18:51:32.211488Z",
     "iopub.status.busy": "2025-03-13T18:51:32.211058Z",
     "iopub.status.idle": "2025-03-13T18:51:39.880632Z",
     "shell.execute_reply": "2025-03-13T18:51:39.879544Z",
     "shell.execute_reply.started": "2025-03-13T18:51:32.211446Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment\n",
      "Positive    364\n",
      "Negative    290\n",
      "Neutral      78\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "ref_words = {\n",
    "    \"Positive\": \"positive\",\n",
    "    \"Negative\": \"negative\",\n",
    "    \"Neutral\": \"neutral\"\n",
    "}\n",
    "\n",
    "ref_vectors = {category: nlp(word).vector for category, word in ref_words.items()}\n",
    "\n",
    "def assign_sentiment_category(sentiment):\n",
    "    word_vector = nlp(sentiment).vector.reshape(1, -1)\n",
    "    \n",
    "    similarities = {}\n",
    "    for category, ref_vec in ref_vectors.items():\n",
    "        ref_vec = ref_vec.reshape(1, -1)\n",
    "        sim = cosine_similarity(word_vector, ref_vec)[0][0]\n",
    "        similarities[category] = sim\n",
    "\n",
    "    return max(similarities, key=similarities.get)\n",
    "\n",
    "train_df['Sentiment'] = train_df['Sentiment'].apply(assign_sentiment_category)\n",
    "\n",
    "print(train_df['Sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T18:51:39.882123Z",
     "iopub.status.busy": "2025-03-13T18:51:39.881761Z",
     "iopub.status.idle": "2025-03-13T18:51:44.814667Z",
     "shell.execute_reply": "2025-03-13T18:51:44.812922Z",
     "shell.execute_reply.started": "2025-03-13T18:51:39.882083Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.2.4)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from nltk) (1.17.0)\n",
      "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "0                             [enjoy, beauti, day, park]\n",
      "1                      [traffic, wa, terribl, thi, morn]\n",
      "2                                [finish, amaz, workout]\n",
      "3                       [excit, upcom, weekend, getaway]\n",
      "4                     [tri, new, recip, dinner, tonight]\n",
      "5                      [feel, grate, littl, thing, life]\n",
      "6          [raini, day, call, cozi, blanket, hot, cocoa]\n",
      "7                        [new, movi, releas, must-watch]\n",
      "8                        [polit, discuss, heat, timelin]\n",
      "9                       [miss, summer, vibe, beach, day]\n",
      "10                     [publish, new, blog, post, check]\n",
      "11                           [feel, bit, weather, today]\n",
      "12                          [explor, city', hidden, gem]\n",
      "13                           [new, year, new, fit, goal]\n",
      "14                         [technolog, chang, way, live]\n",
      "15                          [reflect, past, look, ahead]\n",
      "16                          [adopt, cute, furri, friend]\n",
      "17                   [late-night, game, session, friend]\n",
      "18                         [attend, virtual, confer, ai]\n",
      "19                        [winter, blue, got, feel, low]\n",
      "20                       [sip, coffe, enjoy, good, book]\n",
      "21                     [explor, world, virtual, realiti]\n",
      "22                     [product, day, tick, to-do, list]\n",
      "23                   [finish, challeng, workout, routin]\n",
      "24                              [celebr, mileston, work]\n",
      "25                              [sunday, brunch, friend]\n",
      "26                 [learn, new, languag, person, growth]\n",
      "27                             [quiet, even, good, book]\n",
      "28                     [reflect, import, mental, health]\n",
      "29                                [new, paint, progress]\n",
      "30           [weekend, road, trip, explor, scenic, view]\n",
      "31                      [enjoy, cup, tea, watch, sunset]\n",
      "32                      [code, new, project, enthusiasm]\n",
      "33                      [feel, inspir, attend, workshop]\n",
      "34                     [winter, sport, day, local, park]\n",
      "35                 [qualiti, time, famili, thi, weekend]\n",
      "36               [attend, live, music, concert, tonight]\n",
      "37                                [practic, mind, medit]\n",
      "38                            [tri, new, dessert, recip]\n",
      "39                      [excit, upcom, game, tournament]\n",
      "40                        [plan, garden, makeov, spring]\n",
      "41                  [celebr, friend', birthday, tonight]\n",
      "42                      [feel, accomplish, product, day]\n",
      "43                              [cozi, even, good, movi]\n",
      "44           [explor, local, art, galleri, thi, weekend]\n",
      "45                  [new, book, releas, favorit, author]\n",
      "46                    [attend, virtual, realiti, meetup]\n",
      "47                              [reflect, beauti, natur]\n",
      "48                    [cook, special, dinner, love, one]\n",
      "49                         [feel, optimist, week, ahead]\n",
      "50                 [start, new, fit, challeng, tomorrow]\n",
      "51                   [sunday, bike, ride, scenic, trail]\n",
      "52            [can't, believ, injustic, happen, societi]\n",
      "53             [feel, sens, fear, watch, thriller, movi]\n",
      "54              [heartbroken, hear, news, natur, disast]\n",
      "55                     [state, world', environ, disgust]\n",
      "56             [pure, happi, celebr, love, one', achiev]\n",
      "57        [laughter, best, medicine—enjoy, comedi, show]\n",
      "58                   [share, love, posit, vibe, everyon]\n",
      "59                          [amus, incid, brighten, day]\n",
      "60                       [enjoy, quiet, even, book, tea]\n",
      "61              [admir, beauti, natur, dure, peac, hike]\n",
      "62                       [send, affection, vibe, follow]\n",
      "63                   [experienc, awe, breathtak, sunset]\n",
      "64                  [disappoint, servic, local, restaur]\n",
      "65                    [surpris, gift, friend, made, day]\n",
      "66                [find, accept, midst, life', challeng]\n",
      "67                           [overflow, ador, ador, pet]\n",
      "68               [anticip, thrill, adventur, come, week]\n",
      "69               [bitter, experi, turn, valuabl, lesson]\n",
      "70                        [find, calm, midst, busi, day]\n",
      "71                   [confus, cloud, mind, navig, decis]\n",
      "72                          [excit, build, upcom, vacat]\n",
      "73              [kind, wit, today, restor, faith, human]\n",
      "74                     [pride, achiev, person, mileston]\n",
      "75                      [moment, shame, stand, injustic]\n",
      "76                         [fume, anger, heat, argument]\n",
      "77                          [fear, unknown, keep, night]\n",
      "78          [heartfelt, sad, bid, farewel, dear, friend]\n",
      "79           [state, corrupt, societi, utterli, disgust]\n",
      "80        [overflow, happi, welcom, new, famili, member]\n",
      "81     [laughter, key, joy—attend, stand-up, comedi, ...\n",
      "82                [send, love, follow, thi, beauti, day]\n",
      "83                    [amus, antic, pet—it', pure, amus]\n",
      "84          [enjoy, everi, moment, thi, trip—pur, enjoy]\n",
      "85                [admir, dedic, volunt, local, chariti]\n",
      "86               [send, affection, vibe, friend, famili]\n",
      "87                      [awe-struck, beauti, night, sky]\n",
      "88         [disappoint, lack, progress, person, project]\n",
      "89     [surpris, visit, old, friend, brought, tear, joy]\n",
      "90                               [embrac, accept, life']\n",
      "91                  [overflow, ador, cute, rescu, puppi]\n",
      "92                   [anticip, releas, much-await, movi]\n",
      "93              [bitter, experi, custom, servic, depart]\n",
      "94               [find, calm, amidst, chao, daili, life]\n",
      "95       [confus, reign, tri, make, sens, recent, event]\n",
      "96              [excit, build, surpris, birthday, parti]\n",
      "97                           [wit, act, kind, made, day]\n",
      "98             [pride, complet, challeng, fit, challeng]\n",
      "99                      [moment, shame, speak, injustic]\n",
      "100                     [reflect, beauti, divers, world]\n",
      "101                     [excit, quiet, even, good, book]\n",
      "102                     [feel, bitter, unfair, workplac]\n",
      "103                       [calm, prevail, practic, mind]\n",
      "104              [confus, surround, navig, life', choic]\n",
      "105     [excit, weekend, road, trip, explor, new, place]\n",
      "106             [kind, wit, today, restor, faith, human]\n",
      "107        [pride, accomplish, person, profession, goal]\n",
      "108               [shame, true, valu, difficult, situat]\n",
      "109             [revisit, old, memori, feel, sens, elat]\n",
      "110             [victori, team, brought, euphoria, citi]\n",
      "111             [embrac, beauti, natur, moment, content]\n",
      "112              [medit, seren, lake, find, inner, peac]\n",
      "113                   [overflow, gratitud, life', bless]\n",
      "114         [hope, brighter, tomorrow, despit, challeng]\n",
      "115                        [empow, make, differ, commun]\n",
      "116    [compass, action, support, local, chariti, event]\n",
      "117                 [moment, tender, connect, love, one]\n",
      "118              [arous, excit, await, special, announc]\n",
      "119                     [enthusiast, dive, new, project]\n",
      "120                [feel, sens, fulfil, reach, mileston]\n",
      "121                    [rever, beauti, histor, landmark]\n",
      "122                [elat, surpris, reunion, old, friend]\n",
      "123               [euphoria, live, music, concert, star]\n",
      "124                    [content, simplic, quiet, sunday]\n",
      "125                  [seren, found, page, favorit, book]\n",
      "126       [gratitud, support, receiv, dure, tough, time]\n",
      "127                        [hope, possibl, new, journey]\n",
      "128                     [empower, learn, person, growth]\n",
      "129               [compass, toward, need, dure, holiday]\n",
      "130                 [tender, warmth, cozi, winter, even]\n",
      "131                      [arous, excit, upcom, adventur]\n",
      "132                 [enthusiasm, creativ, project, make]\n",
      "133         [fulfil, complet, challeng, workout, routin]\n",
      "134                   [rever, artistri, display, museum]\n",
      "135                         [elat, achiev, person, goal]\n",
      "136                    [elat, discov, hidden, gem, citi]\n",
      "137                [euphoria, surpris, birthday, celebr]\n",
      "138                  [content, simplic, home-cook, meal]\n",
      "139                  [seren, found, melodi, peac, piano]\n",
      "140                  [gratitud, support, commun, around]\n",
      "141                  [hope, prospect, new, busi, ventur]\n",
      "142                              [empower, mentor, guid]\n",
      "143                  [compass, shown, act, kind, commun]\n",
      "144               [tender, heartfelt, messag, love, one]\n",
      "145              [arous, excit, befor, much-await, trip]\n",
      "146             [enthusiasm, new, artist, project, work]\n",
      "147                           [feel, sens, fulfil, help]\n",
      "148                  [rever, histor, signific, landmark]\n",
      "149                        [elat, achiev, fit, mileston]\n",
      "150                 [euphoria, success, product, launch]\n",
      "151                         [content, embrac, love, one]\n",
      "152                  [seren, found, beauti, sunset, sea]\n",
      "153                   [gratitud, small, joy, day, bring]\n",
      "154                      [hope, potenti, person, growth]\n",
      "155                         [empower, learn, new, skill]\n",
      "156                    [compass, volunt, local, chariti]\n",
      "157                  [tender, quiet, moment, share, pet]\n",
      "158                        [arous, excit, upcom, festiv]\n",
      "159             [enthusiasm, diy, home, improv, project]\n",
      "160                   [fulfil, complet, challeng, puzzl]\n",
      "161                  [rever, wonder, natur, hike, trail]\n",
      "162          [elat, surpris, reunion, childhood, friend]\n",
      "163                    [suffer, despair, anoth, setback]\n",
      "164          [overwhelm, grief, miss, love, one, dearli]\n",
      "165                 [loneli, creep, night, grow, colder]\n",
      "166                     [jealousi, consum, wit, success]\n",
      "167                        [resent, build, past, betray]\n",
      "168              [frustrat, mount, obstacl, block, path]\n",
      "169           [boredom, set, day, feel, endlessli, dull]\n",
      "170        [anxieti, grip, heart, worri, cloud, thought]\n",
      "171                     [intimid, unknown, futur, ahead]\n",
      "172                     [helpless, sink, challeng, pile]\n",
      "173                      [envi, eat, away, see, prosper]\n",
      "174             [regret, miss, opportun, haunt, thought]\n",
      "175                  [disgust, sight, injustic, cruelti]\n",
      "176                 [drown, despair, hope, slip, finger]\n",
      "177     [grief, weigh, heavi, tear, constant, companion]\n",
      "178          [loneli, crowd, room, silent, cri, connect]\n",
      "179                [jealousi, gnaw, confid, toxic, emot]\n",
      "180               [resent, fester, poison, relationship]\n",
      "181                [frustrat, boil, volcan, erupt, emot]\n",
      "182     [boredom, settl, like, dust, life, feel, mundan]\n",
      "183    [anxieti, grip, chest, relentless, grip, thought]\n",
      "184         [intimid, challeng, ahead, fear, take, hold]\n",
      "185              [helpless, engulf, drown, sea, problem]\n",
      "186              [envi, poison, thought, covet, success]\n",
      "187                  [regret, decis, led, pain, present]\n",
      "188                   [disgust, corrupt, stain, societi]\n",
      "189                   [sink, despair, day, darker, last]\n",
      "190              [grief, overwhelm, storm, emot, within]\n",
      "191         [loneli, echo, empti, space, yearn, connect]\n",
      "192    [jealousi, poison, thought, resent, brew, within]\n",
      "193                 [resent, fester, wound, refus, heal]\n",
      "194                [frustrat, escal, thunderstorm, emot]\n",
      "195          [boredom, linger, stagnant, pool, indiffer]\n",
      "196    [embark, journey, discoveri, fuel, curios, thi...\n",
      "197    [lost, vast, sea, inform, indiffer, wave, digi...\n",
      "198    [complex, puzzl, life, leaf, state, perpetu, c...\n",
      "199    [numb, settl, shield, overwhelm, emot, life, t...\n",
      "200    [gaze, sunset, melanchol, long, moment, slip, ...\n",
      "201    [revisit, old, photograph, caught, embrac, nos...\n",
      "202    [torn, conflict, emot, ambival, paint, decis, ...\n",
      "203    [embrac, ebb, flow, life, find, accept, danc, ...\n",
      "204    [face, challeng, head-on, determin, fuel, fire...\n",
      "205    [seren, found, still, natur, tranquil, retreat...\n",
      "206    [curios, lead, rabbit, hole, knowledg, perpetu...\n",
      "207    [float, day, air, indiffer, detach, mundan, ha...\n",
      "208    [entangl, web, thought, confus, reign, navig, ...\n",
      "209    [numb, chao, emot, lock, away, stoic, facad, c...\n",
      "210    [melancholi, whisper, breez, silent, convers, ...\n",
      "211    [stumbl, upon, old, journal, nostalgia, flood,...\n",
      "212    [tapestri, conflict, feel, weav, uncertainti, ...\n",
      "213    [embrac, flaw, find, accept, imperfect, journe...\n",
      "214    [determin, burn, like, wildfir, overcom, obsta...\n",
      "215    [tranquil, moment, ocean, seren, wash, peac, r...\n",
      "216    [explor, new, horizon, spark, curios, adventur...\n",
      "217    [drift, day, nonchal, demeanor, embrac, art, i...\n",
      "218    [wrestl, thought, perplex, mind, lost, labyrin...\n",
      "219    [immers, state, emot, numb, shield, storm, dai...\n",
      "220    [melanchol, symphoni, play, background, soundt...\n",
      "221    [flip, page, old, yearbook, nostalgia, paint, ...\n",
      "222    [torn, oppos, emot, ambival, color, decis, sha...\n",
      "223    [embrac, life', imperfect, find, accept, journ...\n",
      "224    [fieri, determin, burn, within, fuel, vision, ...\n",
      "225    [bask, seren, quiet, forest, whisper, natur, b...\n",
      "226    [indiffer, nois, world, silent, observ, midst,...\n",
      "227    [navig, labyrinth, thought, confus, constant, ...\n",
      "228    [impenetr, numb, shield, emot, storm, fortress...\n",
      "229    [melancholi, paint, world, hue, nostalgia, can...\n",
      "230    [journey, past, flip, page, old, diari, nostal...\n",
      "231    [ambival, cloud, decis, caught, crossroad, con...\n",
      "232    [embrac, imperfect, find, accept, mosaic, life...\n",
      "233    [determin, drive, forc, propel, forward, path,...\n",
      "234    [seek, seren, melodi, raindrop, tranquil, esca...\n",
      "235    [curios, drive, explor, unknown, seeker, knowl...\n",
      "236    [drift, day, air, nonchal, indiffer, trivial, ...\n",
      "237    [lost, labyrinth, thought, confus, cast, shado...\n",
      "238    [wrap, cloak, emot, numb, shield, storm, life'...\n",
      "239    [melancholi, companion, paint, canva, life, br...\n",
      "240    [leaf, page, old, photo, album, nostalgia, wea...\n",
      "241    [ambival, air, caught, crossroad, conflict, em...\n",
      "242    [embrac, beauti, imperfect, find, accept, mosa...\n",
      "243    [determin, ablaz, forg, path, challeng, sculpt...\n",
      "244    [immers, seren, moonlit, night, quiet, whisper...\n",
      "245    [fuel, curios, ventur, unchart, realm, fearles...\n",
      "246    [wrap, cloak, emot, numb, shield, storm, life'...\n",
      "247    [danc, life, exuber, carefre, spirit, embrac, ...\n",
      "248    [bask, golden, glow, content, seren, river, fl...\n",
      "249    [gaze, toward, horizon, hope, eye, paint, canv...\n",
      "250    [stand, tall, proud, oak, branch, achiev, reac...\n",
      "251    [heart, overflow, gratitud, garden, appreci, b...\n",
      "252    [extend, hand, empathet, thread, weav, tapestr...\n",
      "253    [compassion, cloud, heavi, care, shower, empat...\n",
      "254    [play, danc, rain, laughter, whimsic, spirit, ...\n",
      "255    [soar, wing, free, spirit, unburden, chain, co...\n",
      "256    [bath, glow, inspir, creativ, phoenix, rise, a...\n",
      "257    [navig, sea, hope, sail, toward, sunris, possi...\n",
      "258    [stride, confid, footprint, self-assured, impr...\n",
      "259    [lost, symphoni, night, moonlit, serenad, whis...\n",
      "260    [unveil, layer, curios, labyrinth, question, l...\n",
      "261    [embrac, autumn, breez, leaf, ambival, danc, w...\n",
      "262    [gratitud, guid, star, navig, constel, bless, ...\n",
      "263    [zest, heart, sprint, field, enthusiasm, chase...\n",
      "264    [compassion, rain, tear, empathi, fall, gentli...\n",
      "265    [proudli, scale, peak, achiev, mountain, conqu...\n",
      "266    [embrac, hope, dawn, garden, sow, seed, optim,...\n",
      "267    [play, escapad, carniv, life, carousel, laught...\n",
      "268    [float, cloud, inspir, artist, paint, sky, str...\n",
      "269    [navig, river, content, seren, boat, cruis, tr...\n",
      "270    [empathi, lantern, wander, dark, alley, sorrow...\n",
      "271    [free, spirit, soar, wing, dream, leav, trail,...\n",
      "272    [bath, golden, hue, grate, sunset, appreci, ca...\n",
      "273    [confid, stride, danc, life, ballroom, self-as...\n",
      "274    [hope, whisper, wind, carri, promis, brighter,...\n",
      "275    [play, juggl, respons, circu, perform, balanc,...\n",
      "276    [whisper, tale, inspir, star, storytel, craft,...\n",
      "277    [chart, cours, wave, hope, anticip, sailor, st...\n",
      "278    [compassion, rain, tear, empathi, fall, gentli...\n",
      "279    [proudli, scale, peak, achiev, mountain, conqu...\n",
      "280    [embrac, hope, dawn, garden, sow, seed, optim,...\n",
      "281    [play, escapad, carniv, life, carousel, laught...\n",
      "282    [float, cloud, inspir, artist, paint, sky, str...\n",
      "283    [navig, river, content, seren, boat, cruis, tr...\n",
      "284    [empathi, lantern, wander, dark, alley, sorrow...\n",
      "285    [free, spirit, soar, wing, dream, leav, trail,...\n",
      "286    [bath, golden, hue, grate, sunset, appreci, ca...\n",
      "287    [confid, stride, danc, life, ballroom, self-as...\n",
      "288    [hope, whisper, wind, carri, promis, brighter,...\n",
      "289    [play, juggl, respons, circu, perform, balanc,...\n",
      "290    [whisper, tale, inspir, star, storytel, craft,...\n",
      "291    [chart, cours, wave, hope, anticip, sailor, st...\n",
      "292    [compassion, rain, tear, empathi, fall, gentli...\n",
      "293    [proudli, scale, peak, achiev, mountain, conqu...\n",
      "294    [embrac, hope, dawn, garden, sow, seed, optim,...\n",
      "295    [play, escapad, carniv, life, carousel, laught...\n",
      "296    [drown, abyss, despair, heart, shatter, fragme...\n",
      "297    [bitter, fester, like, venom, vine, entwin, so...\n",
      "298    [wander, desert, loneli, step, heavi, sigh, mi...\n",
      "299    [yearn, touch, that', echo, distant, warmth, h...\n",
      "300    [eye, wide, open, night, fear, shadow, danc, w...\n",
      "301    [apprehens, step, tightrop, uncertainti, balan...\n",
      "302    [overwhelm, weight, world, atla, trembl, shoul...\n",
      "303    [jealousi, green-ey, monster, lurk, shadow, ca...\n",
      "304    [devast, storm, betray, wreckag, trust, scatte...\n",
      "305    [frustrat, finger, tap, keyboard, symphoni, an...\n",
      "306    [enviou, eye, fixat, gild, prize, heartach, fu...\n",
      "307    [dismiss, glanc, fortress, built, indiffer, wa...\n",
      "308    [shatter, dream, lie, floor, like, fragment, g...\n",
      "309    [loneli, silent, companion, night, onli, echo,...\n",
      "310    [fear, whisper, dark, mind, haunt, specter, un...\n",
      "311    [bitter, bitter, aftertast, linger, tongu, wor...\n",
      "312    [overwhelm, cacophoni, expect, drown, soul, te...\n",
      "313    [jealousi, venom, seep, vein, poison, heart, t...\n",
      "314    [devast, revel, betray, trust, shatter, like, ...\n",
      "315    [frustrat, attempt, mend, broken, connect, thr...\n",
      "316    [enviou, gaze, cast, upon, podium, success, bi...\n",
      "317    [dismiss, gestur, curtain, drawn, shield, vuln...\n",
      "318    [despair, like, heavi, fog, envelop, everi, th...\n",
      "319    [bitter, bitter, chill, air, freez, moment, ic...\n",
      "320    [loneli, solitari, moon, starless, sky, cast, ...\n",
      "321    [yearn, warmth, vanish, sun, heartach, paint, ...\n",
      "322    [fear, eye, scan, shadow, prison, night, terro...\n",
      "323    [apprehens, whisper, wind, forecast, uncertain...\n",
      "324    [overwhelm, maze, expect, minotaur, pressur, l...\n",
      "325    [jealousi, fester, wound, pain, intensifi, gla...\n",
      "326    [devast, heart, ruin, echo, shatter, dream, re...\n",
      "327    [frustrat, attempt, untangl, knot, confus, thr...\n",
      "328    [enviou, eye, lock, treasur, chest, opportun, ...\n",
      "329    [dismiss, gestur, curtain, drawn, shield, vuln...\n",
      "330    [shatter, dream, lie, floor, like, fragment, g...\n",
      "331    [loneli, silent, companion, night, onli, echo,...\n",
      "332            [awe-struck, breathtak, sunris, mountain]\n",
      "333                          [navig, challeng, determin]\n",
      "334            [nostalgia, hit, flip, old, photo, album]\n",
      "335              [thrill, wit, grandeur, cultur, festiv]\n",
      "336           [calm, found, rhythm, raindrop, windowpan]\n",
      "337    [overwhelm, support, receiv, dure, person, cha...\n",
      "338    [excit, build, countdown, long-await, vacat, b...\n",
      "339      [reflect, life', journey, grate, lesson, learn]\n",
      "340    [bittersweet, emot, aris, bid, farewel, dear, ...\n",
      "341      [curios, spark, explor, mysteri, ancient, ruin]\n",
      "342       [admir, intric, detail, handcraft, masterpiec]\n",
      "343      [overjoy, warmth, cozi, fireplac, winter, even]\n",
      "344     [inspir, strike, observ, color, vibrant, sunset]\n",
      "345         [motiv, achiev, fit, goal, invigor, workout]\n",
      "346      [gratitud, simpl, joy, found, cup, morn, coffe]\n",
      "347        [feel, empow, conquer, challeng, hike, trail]\n",
      "348           [amus, antic, play, kitten, dure, playtim]\n",
      "349       [contempl, life', mysteri, starri, night, sky]\n",
      "350       [joy, reunion, long-lost, friend, year, separ]\n",
      "351              [excit, build, prepar, surpris, celebr]\n",
      "352    [satisfact, deriv, success, complet, diy, proj...\n",
      "353           [feel, bless, support, commun, time, need]\n",
      "354       [captiv, seren, tranquil, garden, full, bloom]\n",
      "355             [anticip, upcom, adventur, exot, destin]\n",
      "356      [reflect, person, growth, achiev, life, experi]\n",
      "357    [nostalg, memori, flood, revisit, childhood, f...\n",
      "358    [appreci, vibrant, cultur, experienc, dure, tr...\n",
      "359      [confid, soar, overcom, public, speak, anxieti]\n",
      "360     [content, midst, famili, gather, fill, laughter]\n",
      "361    [enthusiasm, learn, new, skill, expand, knowledg]\n",
      "362        [surpris, delight, discov, hidden, gem, citi]\n",
      "363       [sens, accomplish, complet, challeng, workout]\n",
      "364               [wonder, beauti, doubl, rainbow, rain]\n",
      "365       [optim, bright, futur, amidst, challeng, time]\n",
      "366    [pride, achiev, person, mileston, career, prog...\n",
      "367     [happi, bloom, like, flower, garden, sunni, day]\n",
      "368         [elat, discov, rare, book, quaint, bookstor]\n",
      "369    [curios, piqu, mysteri, ancient, archaeolog, s...\n",
      "370      [mesmer, cosmic, danc, firefli, moonlit, night]\n",
      "371    [intrigu, symphoni, color, abstract, art, exhi...\n",
      "372    [giggl, joy, echo, air, dure, children', playdat]\n",
      "373                [envelop, seren, practic, mind, lake]\n",
      "374    [chase, dream, like, kite, soar, high, vast, o...\n",
      "375    [spellbound, eleg, ballroom, danc, crystal, ch...\n",
      "376    [whimsic, delight, world, fairi, tale, magic, ...\n",
      "377    [pensiv, contempl, amid, ancient, ruin, forgot...\n",
      "378    [embrac, thrill, speed, rollercoaster', exhila...\n",
      "379    [harmoni, reson, musician, play, melodi, uniti...\n",
      "380    [burst, creativ, quiet, solitud, artist', studio]\n",
      "381    [radiant, joy, akin, bloom, flower, sun-kiss, ...\n",
      "382          [sens, wonder, vast, cosmo, stargaz, night]\n",
      "383         [rejuven, salti, breez, sound, wave, seasid]\n",
      "384        [whisper, inspir, rustl, leaf, seren, forest]\n",
      "385    [savor, warmth, cup, cocoa, chilli, winter, even]\n",
      "386    [heartfelt, gratitud, laughter, share, dure, f...\n",
      "387    [embark, culinari, adventur, savor, exot, flav...\n",
      "388    [euphoria, flood, final, puzzl, piec, click, p...\n",
      "389    [awe-inspir, grandeur, ancient, cathedral', in...\n",
      "390        [captiv, ether, beauti, field, fill, firefli]\n",
      "391    [immers, enchant, melodi, street, musician', v...\n",
      "392         [joy, laughter, reson, live, summer, carniv]\n",
      "393    [explor, univers, within, dure, mind, medit, s...\n",
      "394     [soar, like, free, spirit, wind, coastal, cliff]\n",
      "395       [dazzl, eleg, masquerad, ball', dazzl, costum]\n",
      "396      [whimsic, delight, world, whimsic, fairi, tale]\n",
      "397      [reflect, contempl, amid, ruin, forgotten, era]\n",
      "398    [ride, adrenalin, rush, rollercoaster', wild, ...\n",
      "399    [harmoni, reson, musician, play, symphoni, uniti]\n",
      "400    [burst, artist, creativ, quietud, artist', stu...\n",
      "401    [radiant, joy, akin, blossom, flower, sunlit, ...\n",
      "402            [awe-inspir, vast, cosmo, stargaz, night]\n",
      "403         [rejuven, salti, breez, sound, wave, seasid]\n",
      "404        [whisper, inspir, rustl, leaf, seren, forest]\n",
      "405    [savor, warmth, cup, cocoa, chilli, winter, even]\n",
      "406    [heartfelt, gratitud, laughter, share, dure, f...\n",
      "407    [embark, culinari, odyssey, savor, flavor, aro...\n",
      "408    [euphoria, flood, final, puzzl, piec, fit, per...\n",
      "409    [awe-struck, grandeur, ancient, cathedral', in...\n",
      "410    [curios, awaken, mysteri, ancient, archaeolog,...\n",
      "411           [giddi, excit, first, snowflak, danc, sky]\n",
      "412    [content, envelop, aroma, freshli, bake, bread...\n",
      "413     [inspir, resili, lone, tree, stand, tall, storm]\n",
      "414    [lost, page, captiv, novel, transport, anoth, ...\n",
      "415    [drench, nostalgia, flip, old, famili, photo, ...\n",
      "416    [spark, inspir, ignit, like, shoot, star, nigh...\n",
      "417     [imbu, gratitud, simpl, pleasur, warm, cup, tea]\n",
      "418    [marvel, kaleidoscop, color, vibrant, street, ...\n",
      "419    [awash, seren, sun, set, tranquil, lakesid, re...\n",
      "420        [drown, sorrow, memori, lost, love, resurfac]\n",
      "421           [numb, set, weight, loneli, grow, heavier]\n",
      "422    [tear, fall, like, raindrop, mourn, end, cheri...\n",
      "423    [despair, cloud, mind, feel, adrift, endless, ...\n",
      "424    [shatter, betray, trust, crumbl, like, fragil,...\n",
      "425    [ach, heart, symphoni, pain, play, silenc, sol...\n",
      "426    [emot, storm, whirlwind, sad, engulf, everi, t...\n",
      "427    [haunt, regret, ghost, past, linger, relentles...\n",
      "428     [torn, apart, grief, echo, loss, reverber, soul]\n",
      "429    [isol, deepen, emot, winter, warmth, distant, ...\n",
      "430    [soul-crush, disappoint, hope, shatter, like, ...\n",
      "431    [pain, echo, love, onc, cherish, lost, abyss, ...\n",
      "432    [heartach, deepen, solitari, journey, abyss, d...\n",
      "433    [melancholi, linger, bittersweet, serenad, qui...\n",
      "434    [bitter, like, poison, seep, everi, crevic, wo...\n",
      "435    [emot, exhaust, weight, world, crush, weari, s...\n",
      "436    [sorrow, echo, symphoni, pain, play, string, l...\n",
      "437       [dark, descend, engulf, soul, shadow, despair]\n",
      "438    [desper, whisper, silent, plea, glimmer, hope,...\n",
      "439    [heart, ruin, remnant, shatter, dream, scatter...\n",
      "440    [shatter, echo, shatter, dream, fragment, hope...\n",
      "441    [avoid, thorn, regret, walk, barefoot, path, r...\n",
      "442    [labyrinth, grief, wall, echo, footstep, lost,...\n",
      "443    [soul, adrift, sea, solitud, wave, loneli, cra...\n",
      "444    [bitter, betray, tast, linger, stain, palat, t...\n",
      "445    [ruin, hope, echo, shatter, dream, whisper, ta...\n",
      "446    [sink, like, stone, ocean, heartbreak, rippl, ...\n",
      "447    [tear, ink, stain, page, journal, testament, s...\n",
      "448    [wasteland, lost, trust, echo, broken, promis,...\n",
      "449    [avoid, shard, shatter, dream, walk, tightrop,...\n",
      "450    [suffoc, silenc, solitud, echo, laughter, onc,...\n",
      "451    [haunt, specter, lost, possibl, ghost, refus, ...\n",
      "452    [labyrinth, despair, echo, broken, heart, reve...\n",
      "453    [sink, like, autumn, leaf, river, sorrow, carr...\n",
      "454    [garden, broken, dream, petal, fall, silent, t...\n",
      "455    [tear, currenc, grief, spent, marketplac, lost...\n",
      "456    [wander, maze, betray, wall, close, everi, wro...\n",
      "457    [soul, weather, storm, heartbreak, seek, refug...\n",
      "458    [tapestri, despair, thread, hope, unravel, lea...\n",
      "459    [like, wither, rose, garden, love, petal, fall...\n",
      "460    [void, heartach, echo, love, song, play, note,...\n",
      "461    [nostalgia, bittersweet, danc, moonlit, ballro...\n",
      "462    [symphoni, grief, tear, note, compos, melancho...\n",
      "463    [betray, venom, serpent, slither, garden, trus...\n",
      "464    [sink, quicksand, despair, harder, fight, deep...\n",
      "465    [wander, cemeteri, lost, dream, tombston, mark...\n",
      "466    [swept, away, river, regret, current, past, re...\n",
      "467    [whisper, lost, love, linger, attic, heart, fo...\n",
      "468    [galleri, broken, promis, shatter, vow, frame,...\n",
      "469       [echo, solitud, silent, convers, soul, shadow]\n",
      "470    [danc, sunshin, step, celebr, joy, found, simp...\n",
      "471    [laughter, echo, air, choru, happi, lift, spir...\n",
      "472    [garden, content, bloom, whisper, tale, inner,...\n",
      "473    [chase, dream, vibrant, sky, journey, fuel, ho...\n",
      "474    [serenad, star, heart, full, gratitud, melodi,...\n",
      "475    [bask, glow, accomplish, mileston, step, stone...\n",
      "476    [danc, posit, everi, step, rhythm, uplift, sou...\n",
      "477    [overflow, joy, cup, laughter, share, friend, ...\n",
      "478    [drape, warmth, kind, quilt, compass, stitch, ...\n",
      "479    [garden, friendship, bloom, testament, beauti,...\n",
      "480    [embrac, love, heartbeat, melodi, danc, rhythm...\n",
      "481    [surround, color, joy, canva, paint, laughter,...\n",
      "482    [symphoni, excit, note, burst, energi, ignit, ...\n",
      "483    [surpris, gift, wrap, anticip, unfold, moment,...\n",
      "484    [lost, maze, curios, twist, turn, unveil, trea...\n",
      "485    [float, cloud, gratitud, raindrop, bless, show...\n",
      "486    [like, comet, inspir, streak, sky, creativ, le...\n",
      "487    [celebr, success, firework, accomplish, light,...\n",
      "488    [symphoni, laughter, note, key, unlock, door, ...\n",
      "489    [carniv, emot, rollercoast, thrill, send, hear...\n",
      "490    [stand, befor, grandeur, eiffel, tower, moment...\n",
      "491    [lost, enchant, disneyland, ride, journey, rea...\n",
      "492    [explor, wonder, ferrari, world, roar, engin, ...\n",
      "493    [amidst, tulip, field, keukenhof, tapestri, co...\n",
      "494    [wander, histor, street, kyoto, step, journey,...\n",
      "495    [embrac, grand, canyon, nature', masterpiec, m...\n",
      "496    [journey, seren, santorini, sunset, paint, sky...\n",
      "497    [amaz, architectur, marvel, petra, stone, tell...\n",
      "498    [embark, gondola, ride, venic, canal, reflect,...\n",
      "499    [summit, machu, picchu, breathtak, panorama, w...\n",
      "500    [heart, new, york, citi, time, squar, dazzl, l...\n",
      "501    [captiv, histor, charm, colosseum, stone, echo...\n",
      "502    [sail, azur, water, maldiv, wave, whisper, ser...\n",
      "503    [midst, amazon, rainforest, symphoni, wildlif,...\n",
      "504    [walk, great, wall, china, step, testament, an...\n",
      "505    [summit, mount, fuji, breathtak, sunris, paint...\n",
      "506    [explor, ancient, ruin, angkor, wat, stone, wh...\n",
      "507    [ski, slope, swiss, alp, turn, danc, majesti, ...\n",
      "508    [tranquil, kyoto', bamboo, forest, whisper, an...\n",
      "509    [cruis, fjord, norway, ici, landscap, breathta...\n",
      "510    [front, row, adele', concert, note, 'someon, l...\n",
      "511    [danc, star, beyoncé', live, show, feel, power...\n",
      "512    [crowd, taylor, swift, concert, lyric, 'love, ...\n",
      "513    [rock, guitar, solo, queen, tribut, concert, j...\n",
      "514    [sway, ed, sheeran', acoust, melodi, seren, ev...\n",
      "515    [immers, pulsat, beat, bruno, mar, concert, 'u...\n",
      "516    [michael, jackson, tribut, show, moonwalk, hit...\n",
      "517    [swing, rhythm, frank, sinatra, tribut, feel, ...\n",
      "518    [mosh, pit, metallica, concert, thunder, chord...\n",
      "519    [experienc, magic, coldplay, concert, 'fix, be...\n",
      "520    [justin, bieber, concert, infecti, beat, 'babi...\n",
      "521    [spotlight, ladi, gaga, show, costum, chang, m...\n",
      "522    [immers, soul, melodi, adel, tear, flow, freel...\n",
      "523    [drench, confetti, kati, perri, concert, kalei...\n",
      "524    [audienc, jay-z, perform, lyric, 'empir, state...\n",
      "525    [danc, shakira', rhythmic, beat, hip, sway, hy...\n",
      "526    [u2, concert, anthem, chord, 'with, without, c...\n",
      "527    [rock, gun, n, rose, show, icon, riff, 'sweet,...\n",
      "528    [crowd, ariana, grand, concert, high, note, 'i...\n",
      "529    [sway, regga, vibe, bob, marley', tribut, conc...\n",
      "530    [captiv, spellbind, plot, twist, audienc, appl...\n",
      "531    [credit, roll, profound, sens, nostalgia, wash...\n",
      "532    [stream, latest, web, seri, viewer, engross, c...\n",
      "533    [film, festiv, indi, filmmaker', creation, rec...\n",
      "534    [watch, heartwarm, famili, drama, tear, flow, ...\n",
      "535    [oscar, actor, gracious, accept, award, radiat...\n",
      "536    [discov, hidden, gem, world, documentari, view...\n",
      "537    [movi, credit, roll, viewer, experi, mix, awe,...\n",
      "538    [binge-watch, thrill, crime, seri, suspens, ke...\n",
      "539    [close, scene, unfold, sens, satisfact, wash, ...\n",
      "540    [celebr, histor, victori, world, cup, nation, ...\n",
      "541    [olymp, athlete', persever, shine, earn, gold,...\n",
      "542    [cricket, championship, nail-bit, finish, leaf...\n",
      "543    [wit, record-break, marathon, spectat, fill, a...\n",
      "544    [tenni, grand, slam, fierc, rivalri, unfold, c...\n",
      "545    [cheer, underdog, basketbal, final, crowd, eru...\n",
      "546    [golf, tournament, golfer', precis, focu, lead...\n",
      "547    [experienc, thrill, high-spe, formula, 1, race...\n",
      "548    [cycl, world, championship, climber, conquer, ...\n",
      "549    [wit, heartwarm, comeback, hockey, final, fan,...\n",
      "550    [seri, defeat, soccer, team, face, disappoint,...\n",
      "551    [tenni, tournament, highli, anticip, player, e...\n",
      "552    [face, defeat, championship, boxer, reflect, c...\n",
      "553    [midst, cycl, race, tire, blowout, lead, frust...\n",
      "554    [gymnast', unexpect, fall, dure, routin, spark...\n",
      "555    [golf, tournament, miss, crucial, putt, result...\n",
      "556    [experienc, seri, loss, basketbal, season, tea...\n",
      "557    [despit, meticul, train, swimmer, face, disapp...\n",
      "558    [weightlifter', fail, attempt, person, record,...\n",
      "559    [midst, soccer, match, unexpect, goal, creat, ...\n",
      "560    [seren, beauti, sunset, natur, unfold, canva, ...\n",
      "561    [embark, spontan, road, trip, travel, discov, ...\n",
      "562    [amidst, bustl, citi, quiet, café, becom, sanc...\n",
      "563    [explor, vibrant, street, art, cultur, neighbo...\n",
      "564    [world, scienc, breakthrough, discoveri, unfol...\n",
      "565    [connect, melodi, live, orchestra, music, enth...\n",
      "566    [embrac, aroma, freshli, bake, bread, home, ch...\n",
      "567    [wander, histor, museum, histori, enthusiast, ...\n",
      "568    [realm, literatur, captiv, novel, transport, r...\n",
      "569    [captur, essenc, bustl, market, photograph, fr...\n",
      "570    [underneath, citi, light, dancer, express, emo...\n",
      "571    [heart, bustl, market, street, food, connoisse...\n",
      "572    [first, snowflak, descend, winter, enthusiast,...\n",
      "573    [amidst, page, captiv, mysteri, novel, reader,...\n",
      "574    [surround, vibrant, color, flower, garden, gar...\n",
      "575    [astronomi, observatori, stargaz, marvel, vast...\n",
      "576    [engulf, aroma, freshli, brew, coffe, writer, ...\n",
      "577    [realm, fashion, design, unveil, collect, tell...\n",
      "578    [wave, crash, shore, surfer, embrac, thrill, r...\n",
      "579    [explor, histor, architectur, ancient, citi, t...\n",
      "580    [success, avoid, eye, contact, crush, hallway,...\n",
      "581    [ran, snack, dure, movi, marathon, crisi, leve...\n",
      "582    [spent, hour, choos, perfect, filter, selfi, s...\n",
      "583    [lost, headphon, vanish, thin, air, #headphone...\n",
      "584    [decid, studi, exam, end, make, meme, studi, i...\n",
      "585    [got, dress, day, rememb, it', saturday, oop, ...\n",
      "586    [surviv, group, project, without, ani, drama, ...\n",
      "587    [enter, kitchen, intent, cook, left, bag, chip...\n",
      "588    [stare, clock, class, wait, bell, ring, like, ...\n",
      "589    [discov, new, book, seri, spent, whole, night,...\n",
      "590    [bought, new, video, game, play, hour, forgot,...\n",
      "591    [spent, day, binge-watch, new, seri, product, ...\n",
      "592    [caught, latest, fashion, trend, plan, shop, s...\n",
      "593    [decid, learn, new, instrument, day, one, stil...\n",
      "594    [spent, hour, creat, perfect, playlist, everi,...\n",
      "595    [success, cook, gourmet, meal, famili, chef, s...\n",
      "596    [spontan, book, weekend, getaway, adventur, aw...\n",
      "597    [attend, concert, danc, night, away, music, he...\n",
      "598    [rediscov, childhood, cartoon, nostalgia-fil, ...\n",
      "599    [embark, diy, home, decor, project, let', hope...\n",
      "600    [spent, afternoon, museum, pretend, cultur, ar...\n",
      "601    [start, blog, random, thought, muse, blog, new...\n",
      "602    [relish, peac, afternoon, classic, novel, quie...\n",
      "603    [reflect, lifetim, memori, wrinkl, tell, stori...\n",
      "604    [explor, world, digit, art, it', never, late, ...\n",
      "605    [savor, flavor, home-cook, meal, simpl, joy, h...\n",
      "606    [embark, journey, learn, new, languag, mind, s...\n",
      "607    [attend, classic, music, concert, feel, timele...\n",
      "608    [captur, beauti, natur, photographi, everi, sn...\n",
      "609    [reconnect, old, friend, cup, tea, friendship,...\n",
      "610    [embark, road, trip, revisit, cherish, place, ...\n",
      "611    [join, commun, choir, harmon, fellow, voic, mu...\n",
      "612    [explor, art, medit, find, tranquil, still, mi...\n",
      "613    [take, stroll, garden, appreci, beauti, bloom,...\n",
      "614    [sip, favorit, vintag, wine, sip, tell, stori,...\n",
      "615    [particip, commun, art, class, unleash, creati...\n",
      "616    [embark, journey, write, memoir, document, lif...\n",
      "617    [attend, lectur, histori, alway, fascin, lesso...\n",
      "618    [rediscov, joy, cook, tradit, famili, recip, k...\n",
      "619    [join, natur, photographi, club, captur, beaut...\n",
      "620    [attend, jazz, concert, sway, rhythm, timeless...\n",
      "621    [join, write, group, pen, thought, reflect, wr...\n",
      "622    [embark, solo, travel, adventur, discov, beaut...\n",
      "623    [attend, vintag, car, show, reminisc, classic,...\n",
      "624    [start, commun, garden, grow, plant, friendshi...\n",
      "625    [host, famili, dinner, laughter, echo, louder,...\n",
      "626    [enrol, danc, class, senior, move, rhythm, lif...\n",
      "627    [visit, art, galleri, appreci, brushstrok, tel...\n",
      "628    [start, book, club, senior, discuss, live, cha...\n",
      "629    [host, picnic, park, bask, warmth, friendship,...\n",
      "630    [particip, local, theater, product, prove, sta...\n",
      "631    [embark, hike, adventur, conquer, trail, relis...\n",
      "632    [host, photographi, exhibit, featur, snapshot,...\n",
      "633    [join, senior, cycl, club, feel, wind, hair, f...\n",
      "634    [attend, wine, tast, event, savor, rich, flavo...\n",
      "635    [start, learn, ballroom, danc, glide, grace, a...\n",
      "636    [organ, commun, paint, event, turn, blank, can...\n",
      "637    [host, 'memori, lane, even, old, friend, remin...\n",
      "638    [join, senior, astronomi, club, stargaz, find,...\n",
      "639    [attend, local, jazz, festiv, tap, toe, tune, ...\n",
      "640    [start, blog, share, wisdom, gain, year, prove...\n",
      "641    [particip, chariti, run, prove, age, barrier, ...\n",
      "642      [surviv, challeng, physic, exam, equat, defeat]\n",
      "643    [explor, world, code, debug, adventur, #coding...\n",
      "644    [join, school, debat, team, word, weapon, read...\n",
      "645    [start, photographi, club, school, captur, mom...\n",
      "646    [daydream, upcom, prom, dress, danc, –, it', f...\n",
      "647    [convinc, teacher, class, outdoor, learn, equa...\n",
      "648    [accident, spill, paint, art, class, abstract,...\n",
      "649    [tri, master, perfect, kickflip, skateboard, s...\n",
      "650    [bond, friend, latest, k-pop, sensat, fangirl,...\n",
      "651    [spent, hour, perfect, chemistri, experi, mix,...\n",
      "652    [success, organ, surpris, birthday, parti, fri...\n",
      "653    [join, drama, club, unleash, inner, actor, lig...\n",
      "654    [got, hand, latest, fantasi, novel, dive, real...\n",
      "655    [master, art, perfect, doodl, dure, bore, clas...\n",
      "656    [attempt, break, school, record, longest, hand...\n",
      "657    [sneak, snack, class, like, pro, art, snack-sm...\n",
      "658    [host, sleepov, friend, thi, weekend, prepar, ...\n",
      "659    [spent, hour, tiktok, danc, onli, realiz, two,...\n",
      "660    [accident, like, crush', old, photo, stalk, pr...\n",
      "661    [tri, impress, crush, smooth, convers, end, sp...\n",
      "662    [master, art, creat, paper, airplan, dure, lec...\n",
      "663    [tri, set, new, trend, juggl, textbook, class,...\n",
      "664    [hide, snack, stash, backpack, emerg, crave, s...\n",
      "665    [plan, surpris, scaveng, hunt, friend, anticip...\n",
      "666    [danc, rain, celebr, end, exam, rain, danc, un...\n",
      "667    [accident, sent, text, meant, friend, class, g...\n",
      "668    [tri, magic, trick, impress, classmat, magic, ...\n",
      "669    [perfect, art, creat, origami, dure, dull, lec...\n",
      "670    [attempt, set, new, record, consecut, hacki, s...\n",
      "671    [creat, secret, handshak, friend, friendship, ...\n",
      "672    [embark, mission, find, best, burger, joint, t...\n",
      "673    [practic, stand-up, comedi, routin, upcom, tal...\n",
      "674    [accident, sent, love, letter, wrong, person, ...\n",
      "675    [attempt, impress, teacher, elabor, scienc, ex...\n",
      "676    [craft, intric, friendship, bracelet, whole, s...\n",
      "677    [attempt, beat, record, consecut, cartwheel, c...\n",
      "678    [organ, movi, marathon, friend, popcorn, cinem...\n",
      "679    [experi, new, hair, color, bold, chang, bold, ...\n",
      "680    [build, time, capsul, captur, memori, futur, t...\n",
      "681    [accident, walk, wrong, classroom, first, day,...\n",
      "682    [tri, new, smoothi, recip, healthi, start, wee...\n",
      "683    [reflect, challeng, school, year, feel, bit, o...\n",
      "684    [encount, mean-spirit, comment, onlin, deal, o...\n",
      "685         [bad, day, school, everyth, seem, go, wrong]\n",
      "686        [feel, make, sport, team, disappoint, linger]\n",
      "687    [wit, heat, argument, cafeteria, unpleas, atmo...\n",
      "688    [receiv, not-so-great, grade, major, project, ...\n",
      "689    [deal, person, setback, sometim, life, throw, ...\n",
      "690    [feel, lone, saturday, night, sometim, solitud...\n",
      "691    [experienc, cyberbulli, hate, messag, onlin, d...\n",
      "692    [caught, torrenti, rainstorm, without, umbrell...\n",
      "693    [miss, import, event, due, unforeseen, circums...\n",
      "694    [deal, unfound, rumor, circul, person, life, r...\n",
      "695    [got, flat, tire, way, import, meet, talk, ser...\n",
      "696    [feel, sens, empti, close, friend, move, away,...\n",
      "697    [face, reject, dream, colleg, dishearten, dete...\n",
      "698    [encount, onlin, toxic, dure, game, session, h...\n",
      "699    [bad, hair, day, feel, self-consci, bad, hair,...\n",
      "700    [feel, sens, despair, major, project, failur, ...\n",
      "701    [experienc, hate, comment, express, person, op...\n",
      "702    [string, bad, luck, constant, technolog, malfu...\n",
      "703    [miss, long-anticip, event, due, unexpect, cir...\n",
      "704    [tri, new, studi, techniqu, upcom, exam, explo...\n",
      "705    [organ, commun, cleanup, event, cleaner, neigh...\n",
      "706    [share, favorit, book, recommend, classmat, bu...\n",
      "707    [experi, new, recip, school, bake, sale, bake,...\n",
      "708    [collabor, school, project, peer, teamwork, ma...\n",
      "709    [attend, school, club, meet, explor, new, inte...\n",
      "710    [explor, new, part-tim, job, opportun, gain, w...\n",
      "711    [attend, school, assembl, stay, inform, upcom,...\n",
      "712    [explor, new, hobbi, photographi, dure, free, ...\n",
      "713    [particip, scienc, fair, showcas, uniqu, exper...\n",
      "714    [attend, workshop, time, manag, enhanc, organi...\n",
      "715    [volunt, local, chariti, event, give, back, co...\n",
      "716    [collabor, group, project, promot, teamwork, s...\n",
      "717    [particip, debat, club, enhanc, critic, think,...\n",
      "718    [celebr, friend', birthday, surpris, parti, jo...\n",
      "719    [success, complet, challeng, code, project, ex...\n",
      "720    [attend, school, talent, show, support, classm...\n",
      "721    [explor, new, hike, trail, friend, weekend, na...\n",
      "722    [win, friendli, sport, competit, rival, school...\n",
      "723    [receiv, heartfelt, letter, pen, pal, anoth, c...\n",
      "724    [creat, beauti, mural, fellow, art, enthusiast...\n",
      "725    [particip, school-wid, art, exhibit, wit, crea...\n",
      "726    [achiev, person, best, track, field, competit,...\n",
      "727    [collabor, scienc, project, receiv, recognit, ...\n",
      "728    [attend, surpris, birthday, parti, organ, frie...\n",
      "729    [success, fundrais, school, chariti, initi, jo...\n",
      "730    [particip, multicultur, festiv, celebr, divers...\n",
      "731    [organ, virtual, talent, show, dure, challeng,...\n",
      "Name: Text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk  \n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')  \n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  \n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  \n",
    "                           u\"\\U0001F680-\\U0001F6FF\"\n",
    "                           u\"\\U0001F700-\\U0001F77F\" \n",
    "                           u\"\\U0001F780-\\U0001F7FF\"  \n",
    "                           u\"\\U0001F800-\\U0001F8FF\" \n",
    "                           u\"\\U0001F900-\\U0001F9FF\" \n",
    "                           u\"\\U0001FA00-\\U0001FA6F\" \n",
    "                           u\"\\U0001FA70-\\U0001FAFF\" \n",
    "                           u\"\\U00002702-\\U000027B0\" \n",
    "                           u\"\\U000024C2-\\U0001F251\" \n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "\n",
    "def split_and_remove_stopwords(text):\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "    tokens = text.split()\n",
    "\n",
    "    filtered_tokens = [\n",
    "        stemmer.stem(lemmatizer.lemmatize(word.lower().rstrip(string.punctuation)))\n",
    "        for word in tokens if stemmer.stem(word.lower().rstrip(string.punctuation)) not in stop_words\n",
    "    ]\n",
    "\n",
    "    return filtered_tokens\n",
    "\n",
    "train_df['Text'] = train_df['Text'].apply(split_and_remove_stopwords)\n",
    "\n",
    "print(train_df['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T19:02:28.067551Z",
     "iopub.status.busy": "2025-03-13T19:02:28.067028Z",
     "iopub.status.idle": "2025-03-13T19:08:09.348774Z",
     "shell.execute_reply": "2025-03-13T19:08:09.347486Z",
     "shell.execute_reply.started": "2025-03-13T19:02:28.067514Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 91ms/step - accuracy: 0.3717 - loss: 1.6935 - val_accuracy: 0.4762 - val_loss: 1.0583 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - accuracy: 0.5586 - loss: 1.3134 - val_accuracy: 0.4762 - val_loss: 1.1228 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.5615 - loss: 1.2757 - val_accuracy: 0.5034 - val_loss: 1.1097 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.6105 - loss: 1.1275\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - accuracy: 0.6154 - loss: 1.1107 - val_accuracy: 0.4898 - val_loss: 1.1243 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - accuracy: 0.6182 - loss: 0.9780 - val_accuracy: 0.4966 - val_loss: 1.1229 - learning_rate: 5.0000e-04\n",
      "Epoch 6/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - accuracy: 0.6430 - loss: 0.9441 - val_accuracy: 0.5102 - val_loss: 1.0375 - learning_rate: 5.0000e-04\n",
      "Epoch 7/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - accuracy: 0.6892 - loss: 0.7999 - val_accuracy: 0.5034 - val_loss: 1.0429 - learning_rate: 5.0000e-04\n",
      "Epoch 8/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - accuracy: 0.6439 - loss: 0.9680 - val_accuracy: 0.5102 - val_loss: 1.0065 - learning_rate: 5.0000e-04\n",
      "Epoch 9/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 77ms/step - accuracy: 0.6647 - loss: 0.8773 - val_accuracy: 0.5238 - val_loss: 1.0005 - learning_rate: 5.0000e-04\n",
      "Epoch 10/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - accuracy: 0.6492 - loss: 0.9042 - val_accuracy: 0.5238 - val_loss: 0.9488 - learning_rate: 5.0000e-04\n",
      "Epoch 11/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - accuracy: 0.6489 - loss: 0.9036 - val_accuracy: 0.5714 - val_loss: 0.8964 - learning_rate: 5.0000e-04\n",
      "Epoch 12/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - accuracy: 0.6937 - loss: 0.7781 - val_accuracy: 0.6122 - val_loss: 0.8762 - learning_rate: 5.0000e-04\n",
      "Epoch 13/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - accuracy: 0.7169 - loss: 0.7606 - val_accuracy: 0.5918 - val_loss: 0.9076 - learning_rate: 5.0000e-04\n",
      "Epoch 14/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - accuracy: 0.6994 - loss: 0.7364 - val_accuracy: 0.5986 - val_loss: 0.8946 - learning_rate: 5.0000e-04\n",
      "Epoch 15/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.7176 - loss: 0.7151 - val_accuracy: 0.6190 - val_loss: 0.8631 - learning_rate: 5.0000e-04\n",
      "Epoch 16/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - accuracy: 0.7054 - loss: 0.6892 - val_accuracy: 0.6190 - val_loss: 0.8637 - learning_rate: 5.0000e-04\n",
      "Epoch 17/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.7150 - loss: 0.7165 - val_accuracy: 0.6327 - val_loss: 0.8614 - learning_rate: 5.0000e-04\n",
      "Epoch 18/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.6918 - loss: 0.7602 - val_accuracy: 0.6463 - val_loss: 0.8529 - learning_rate: 5.0000e-04\n",
      "Epoch 19/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - accuracy: 0.7623 - loss: 0.6073 - val_accuracy: 0.6327 - val_loss: 0.8610 - learning_rate: 5.0000e-04\n",
      "Epoch 20/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.7418 - loss: 0.6847 - val_accuracy: 0.6259 - val_loss: 0.8627 - learning_rate: 5.0000e-04\n",
      "Epoch 21/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.7375 - loss: 0.6524 - val_accuracy: 0.6531 - val_loss: 0.8271 - learning_rate: 5.0000e-04\n",
      "Epoch 22/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.7608 - loss: 0.6469 - val_accuracy: 0.6871 - val_loss: 0.8267 - learning_rate: 5.0000e-04\n",
      "Epoch 23/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - accuracy: 0.7496 - loss: 0.6442 - val_accuracy: 0.6939 - val_loss: 0.8222 - learning_rate: 5.0000e-04\n",
      "Epoch 24/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - accuracy: 0.7499 - loss: 0.6029 - val_accuracy: 0.6939 - val_loss: 0.7921 - learning_rate: 5.0000e-04\n",
      "Epoch 25/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - accuracy: 0.7722 - loss: 0.6020 - val_accuracy: 0.6871 - val_loss: 0.7761 - learning_rate: 5.0000e-04\n",
      "Epoch 26/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.7585 - loss: 0.5984 - val_accuracy: 0.7007 - val_loss: 0.7867 - learning_rate: 5.0000e-04\n",
      "Epoch 27/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - accuracy: 0.7914 - loss: 0.5289 - val_accuracy: 0.7007 - val_loss: 0.8137 - learning_rate: 5.0000e-04\n",
      "Epoch 28/100\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.7501 - loss: 0.5742\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.7498 - loss: 0.5760 - val_accuracy: 0.7007 - val_loss: 0.8394 - learning_rate: 5.0000e-04\n",
      "Epoch 29/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.7934 - loss: 0.5410 - val_accuracy: 0.7143 - val_loss: 0.8285 - learning_rate: 2.5000e-04\n",
      "Epoch 30/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.7959 - loss: 0.5019 - val_accuracy: 0.7143 - val_loss: 0.8183 - learning_rate: 2.5000e-04\n",
      "Epoch 31/100\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.7468 - loss: 0.6406\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - accuracy: 0.7512 - loss: 0.6253 - val_accuracy: 0.7007 - val_loss: 0.8074 - learning_rate: 2.5000e-04\n",
      "Epoch 32/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - accuracy: 0.7607 - loss: 0.5485 - val_accuracy: 0.7143 - val_loss: 0.8023 - learning_rate: 1.2500e-04\n",
      "Epoch 33/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.7880 - loss: 0.5385 - val_accuracy: 0.7143 - val_loss: 0.7999 - learning_rate: 1.2500e-04\n",
      "Epoch 34/100\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.8274 - loss: 0.4492\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.8258 - loss: 0.4567 - val_accuracy: 0.7007 - val_loss: 0.7988 - learning_rate: 1.2500e-04\n",
      "Epoch 35/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.8032 - loss: 0.4839 - val_accuracy: 0.7007 - val_loss: 0.7962 - learning_rate: 6.2500e-05\n",
      "Epoch 36/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.8278 - loss: 0.4281 - val_accuracy: 0.7007 - val_loss: 0.7962 - learning_rate: 6.2500e-05\n",
      "Epoch 37/100\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.7966 - loss: 0.4927\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.7950 - loss: 0.4971 - val_accuracy: 0.7007 - val_loss: 0.7984 - learning_rate: 6.2500e-05\n",
      "Epoch 38/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.8130 - loss: 0.4451 - val_accuracy: 0.7075 - val_loss: 0.8000 - learning_rate: 3.1250e-05\n",
      "Epoch 39/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.8080 - loss: 0.5173 - val_accuracy: 0.7075 - val_loss: 0.8034 - learning_rate: 3.1250e-05\n",
      "Epoch 40/100\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.7654 - loss: 0.5273\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - accuracy: 0.7683 - loss: 0.5274 - val_accuracy: 0.7143 - val_loss: 0.8066 - learning_rate: 3.1250e-05\n",
      "Epoch 41/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - accuracy: 0.8144 - loss: 0.4464 - val_accuracy: 0.7143 - val_loss: 0.8100 - learning_rate: 1.5625e-05\n",
      "Epoch 42/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.7751 - loss: 0.5177 - val_accuracy: 0.7143 - val_loss: 0.8134 - learning_rate: 1.5625e-05\n",
      "Epoch 43/100\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.8013 - loss: 0.4739\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.8011 - loss: 0.4766 - val_accuracy: 0.7143 - val_loss: 0.8180 - learning_rate: 1.5625e-05\n",
      "Epoch 44/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - accuracy: 0.8016 - loss: 0.4640 - val_accuracy: 0.7143 - val_loss: 0.8212 - learning_rate: 7.8125e-06\n",
      "Epoch 45/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - accuracy: 0.7720 - loss: 0.5271 - val_accuracy: 0.7143 - val_loss: 0.8253 - learning_rate: 7.8125e-06\n",
      "Epoch 46/100\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.7999 - loss: 0.4811\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.7987 - loss: 0.4858 - val_accuracy: 0.7143 - val_loss: 0.8297 - learning_rate: 7.8125e-06\n",
      "Epoch 47/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - accuracy: 0.8077 - loss: 0.4923 - val_accuracy: 0.7143 - val_loss: 0.8342 - learning_rate: 3.9063e-06\n",
      "Epoch 48/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.7955 - loss: 0.4984 - val_accuracy: 0.7143 - val_loss: 0.8378 - learning_rate: 3.9063e-06\n",
      "Epoch 49/100\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.7876 - loss: 0.5318\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - accuracy: 0.7892 - loss: 0.5290 - val_accuracy: 0.7211 - val_loss: 0.8426 - learning_rate: 3.9063e-06\n",
      "Epoch 50/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - accuracy: 0.8293 - loss: 0.4203 - val_accuracy: 0.7347 - val_loss: 0.8469 - learning_rate: 1.9531e-06\n",
      "Epoch 51/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.8066 - loss: 0.4906 - val_accuracy: 0.7415 - val_loss: 0.8509 - learning_rate: 1.9531e-06\n",
      "Epoch 52/100\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.7880 - loss: 0.5002\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - accuracy: 0.7880 - loss: 0.5023 - val_accuracy: 0.7415 - val_loss: 0.8561 - learning_rate: 1.9531e-06\n",
      "Epoch 53/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - accuracy: 0.8147 - loss: 0.4870 - val_accuracy: 0.7415 - val_loss: 0.8586 - learning_rate: 1.0000e-06\n",
      "Epoch 54/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - accuracy: 0.8037 - loss: 0.4978 - val_accuracy: 0.7415 - val_loss: 0.8625 - learning_rate: 1.0000e-06\n",
      "Epoch 55/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - accuracy: 0.7962 - loss: 0.4455 - val_accuracy: 0.7483 - val_loss: 0.8662 - learning_rate: 1.0000e-06\n",
      "Epoch 56/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - accuracy: 0.8108 - loss: 0.4723 - val_accuracy: 0.7483 - val_loss: 0.8701 - learning_rate: 1.0000e-06\n",
      "Epoch 57/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - accuracy: 0.8163 - loss: 0.4765 - val_accuracy: 0.7483 - val_loss: 0.8753 - learning_rate: 1.0000e-06\n",
      "Epoch 58/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - accuracy: 0.8113 - loss: 0.4765 - val_accuracy: 0.7415 - val_loss: 0.8779 - learning_rate: 1.0000e-06\n",
      "Epoch 59/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - accuracy: 0.7562 - loss: 0.5143 - val_accuracy: 0.7415 - val_loss: 0.8831 - learning_rate: 1.0000e-06\n",
      "Epoch 60/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 82ms/step - accuracy: 0.8274 - loss: 0.4571 - val_accuracy: 0.7415 - val_loss: 0.8858 - learning_rate: 1.0000e-06\n",
      "Epoch 61/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - accuracy: 0.7729 - loss: 0.5669 - val_accuracy: 0.7415 - val_loss: 0.8895 - learning_rate: 1.0000e-06\n",
      "Epoch 62/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - accuracy: 0.7957 - loss: 0.5184 - val_accuracy: 0.7415 - val_loss: 0.8935 - learning_rate: 1.0000e-06\n",
      "Epoch 63/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - accuracy: 0.8017 - loss: 0.4959 - val_accuracy: 0.7415 - val_loss: 0.8979 - learning_rate: 1.0000e-06\n",
      "Epoch 64/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - accuracy: 0.8145 - loss: 0.4604 - val_accuracy: 0.7415 - val_loss: 0.9015 - learning_rate: 1.0000e-06\n",
      "Epoch 65/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - accuracy: 0.8349 - loss: 0.4408 - val_accuracy: 0.7415 - val_loss: 0.9059 - learning_rate: 1.0000e-06\n",
      "Epoch 66/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - accuracy: 0.8286 - loss: 0.4740 - val_accuracy: 0.7415 - val_loss: 0.9086 - learning_rate: 1.0000e-06\n",
      "Epoch 67/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - accuracy: 0.7809 - loss: 0.4878 - val_accuracy: 0.7415 - val_loss: 0.9128 - learning_rate: 1.0000e-06\n",
      "Epoch 68/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - accuracy: 0.8278 - loss: 0.4731 - val_accuracy: 0.7415 - val_loss: 0.9159 - learning_rate: 1.0000e-06\n",
      "Epoch 69/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - accuracy: 0.7913 - loss: 0.5057 - val_accuracy: 0.7415 - val_loss: 0.9191 - learning_rate: 1.0000e-06\n",
      "Epoch 70/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - accuracy: 0.7760 - loss: 0.5231 - val_accuracy: 0.7415 - val_loss: 0.9202 - learning_rate: 1.0000e-06\n",
      "Epoch 71/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - accuracy: 0.8216 - loss: 0.4629 - val_accuracy: 0.7415 - val_loss: 0.9237 - learning_rate: 1.0000e-06\n",
      "Epoch 72/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - accuracy: 0.8165 - loss: 0.4634 - val_accuracy: 0.7415 - val_loss: 0.9251 - learning_rate: 1.0000e-06\n",
      "Epoch 73/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - accuracy: 0.8086 - loss: 0.4460 - val_accuracy: 0.7415 - val_loss: 0.9289 - learning_rate: 1.0000e-06\n",
      "Epoch 74/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.8101 - loss: 0.4793 - val_accuracy: 0.7415 - val_loss: 0.9303 - learning_rate: 1.0000e-06\n",
      "Epoch 75/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - accuracy: 0.8303 - loss: 0.4773 - val_accuracy: 0.7483 - val_loss: 0.9308 - learning_rate: 1.0000e-06\n",
      "Epoch 76/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - accuracy: 0.7793 - loss: 0.5363 - val_accuracy: 0.7483 - val_loss: 0.9357 - learning_rate: 1.0000e-06\n",
      "Epoch 77/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - accuracy: 0.8211 - loss: 0.4514 - val_accuracy: 0.7483 - val_loss: 0.9369 - learning_rate: 1.0000e-06\n",
      "Epoch 78/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - accuracy: 0.8034 - loss: 0.4759 - val_accuracy: 0.7483 - val_loss: 0.9367 - learning_rate: 1.0000e-06\n",
      "Epoch 79/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - accuracy: 0.7802 - loss: 0.5109 - val_accuracy: 0.7483 - val_loss: 0.9363 - learning_rate: 1.0000e-06\n",
      "Epoch 80/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - accuracy: 0.7878 - loss: 0.4910 - val_accuracy: 0.7483 - val_loss: 0.9375 - learning_rate: 1.0000e-06\n",
      "Epoch 81/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - accuracy: 0.7971 - loss: 0.4682 - val_accuracy: 0.7483 - val_loss: 0.9372 - learning_rate: 1.0000e-06\n",
      "Epoch 82/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - accuracy: 0.8226 - loss: 0.4420 - val_accuracy: 0.7483 - val_loss: 0.9381 - learning_rate: 1.0000e-06\n",
      "Epoch 83/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.7954 - loss: 0.5057 - val_accuracy: 0.7483 - val_loss: 0.9387 - learning_rate: 1.0000e-06\n",
      "Epoch 84/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.7826 - loss: 0.5012 - val_accuracy: 0.7483 - val_loss: 0.9397 - learning_rate: 1.0000e-06\n",
      "Epoch 85/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - accuracy: 0.8264 - loss: 0.4920 - val_accuracy: 0.7483 - val_loss: 0.9391 - learning_rate: 1.0000e-06\n",
      "Epoch 86/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - accuracy: 0.8164 - loss: 0.4718 - val_accuracy: 0.7415 - val_loss: 0.9386 - learning_rate: 1.0000e-06\n",
      "Epoch 87/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - accuracy: 0.7975 - loss: 0.4877 - val_accuracy: 0.7483 - val_loss: 0.9409 - learning_rate: 1.0000e-06\n",
      "Epoch 88/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - accuracy: 0.8078 - loss: 0.4739 - val_accuracy: 0.7483 - val_loss: 0.9409 - learning_rate: 1.0000e-06\n",
      "Epoch 89/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - accuracy: 0.8216 - loss: 0.4374 - val_accuracy: 0.7483 - val_loss: 0.9430 - learning_rate: 1.0000e-06\n",
      "Epoch 90/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - accuracy: 0.8110 - loss: 0.4776 - val_accuracy: 0.7483 - val_loss: 0.9444 - learning_rate: 1.0000e-06\n",
      "Epoch 91/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - accuracy: 0.7651 - loss: 0.5437 - val_accuracy: 0.7483 - val_loss: 0.9437 - learning_rate: 1.0000e-06\n",
      "Epoch 92/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - accuracy: 0.7700 - loss: 0.5479 - val_accuracy: 0.7483 - val_loss: 0.9449 - learning_rate: 1.0000e-06\n",
      "Epoch 93/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.8437 - loss: 0.4064 - val_accuracy: 0.7483 - val_loss: 0.9450 - learning_rate: 1.0000e-06\n",
      "Epoch 94/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - accuracy: 0.8241 - loss: 0.4213 - val_accuracy: 0.7483 - val_loss: 0.9446 - learning_rate: 1.0000e-06\n",
      "Epoch 95/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.7961 - loss: 0.4888 - val_accuracy: 0.7415 - val_loss: 0.9444 - learning_rate: 1.0000e-06\n",
      "Epoch 96/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - accuracy: 0.7969 - loss: 0.5003 - val_accuracy: 0.7483 - val_loss: 0.9437 - learning_rate: 1.0000e-06\n",
      "Epoch 97/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - accuracy: 0.7838 - loss: 0.5079 - val_accuracy: 0.7483 - val_loss: 0.9444 - learning_rate: 1.0000e-06\n",
      "Epoch 98/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - accuracy: 0.7827 - loss: 0.5102 - val_accuracy: 0.7483 - val_loss: 0.9469 - learning_rate: 1.0000e-06\n",
      "Epoch 99/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.8093 - loss: 0.4955 - val_accuracy: 0.7483 - val_loss: 0.9446 - learning_rate: 1.0000e-06\n",
      "Epoch 100/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - accuracy: 0.8178 - loss: 0.4686 - val_accuracy: 0.7483 - val_loss: 0.9451 - learning_rate: 1.0000e-06\n",
      "Epoch 1/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 90ms/step - accuracy: 0.3889 - loss: 1.6456 - val_accuracy: 0.5510 - val_loss: 1.0039 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.5627 - loss: 1.1977 - val_accuracy: 0.5714 - val_loss: 0.9476 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.5705 - loss: 1.1614 - val_accuracy: 0.6259 - val_loss: 0.9039 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.6371 - loss: 1.0249 - val_accuracy: 0.6054 - val_loss: 0.9044 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.6473 - loss: 0.9468 - val_accuracy: 0.6054 - val_loss: 0.9071 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.6166 - loss: 0.9463 - val_accuracy: 0.5714 - val_loss: 0.8923 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.6498 - loss: 0.8825 - val_accuracy: 0.6463 - val_loss: 0.8255 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - accuracy: 0.6757 - loss: 0.8234 - val_accuracy: 0.6395 - val_loss: 0.8252 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.6552 - loss: 0.9031 - val_accuracy: 0.6531 - val_loss: 0.8121 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.6654 - loss: 0.8279 - val_accuracy: 0.6122 - val_loss: 0.8379 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.6856 - loss: 0.7465 - val_accuracy: 0.6871 - val_loss: 0.7925 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.7061 - loss: 0.7099 - val_accuracy: 0.7075 - val_loss: 0.7680 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - accuracy: 0.6749 - loss: 0.7596 - val_accuracy: 0.6939 - val_loss: 0.7684 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.7110 - loss: 0.7042 - val_accuracy: 0.6395 - val_loss: 0.7907 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.7334 - loss: 0.6687\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.7331 - loss: 0.6716 - val_accuracy: 0.6395 - val_loss: 0.8140 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.7435 - loss: 0.6575 - val_accuracy: 0.6327 - val_loss: 0.8068 - learning_rate: 5.0000e-04\n",
      "Epoch 17/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.7305 - loss: 0.6622 - val_accuracy: 0.6463 - val_loss: 0.7783 - learning_rate: 5.0000e-04\n",
      "Epoch 18/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.7688 - loss: 0.6043 - val_accuracy: 0.6599 - val_loss: 0.7589 - learning_rate: 5.0000e-04\n",
      "Epoch 19/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.7423 - loss: 0.6137 - val_accuracy: 0.6667 - val_loss: 0.7504 - learning_rate: 5.0000e-04\n",
      "Epoch 20/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.7405 - loss: 0.6137 - val_accuracy: 0.6735 - val_loss: 0.7463 - learning_rate: 5.0000e-04\n",
      "Epoch 21/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.7701 - loss: 0.6064 - val_accuracy: 0.6463 - val_loss: 0.7714 - learning_rate: 5.0000e-04\n",
      "Epoch 22/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.7738 - loss: 0.5898 - val_accuracy: 0.6667 - val_loss: 0.7577 - learning_rate: 5.0000e-04\n",
      "Epoch 23/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.7899 - loss: 0.5356 - val_accuracy: 0.6939 - val_loss: 0.7388 - learning_rate: 5.0000e-04\n",
      "Epoch 24/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.7924 - loss: 0.4898 - val_accuracy: 0.7143 - val_loss: 0.7371 - learning_rate: 5.0000e-04\n",
      "Epoch 25/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - accuracy: 0.7646 - loss: 0.5686 - val_accuracy: 0.6939 - val_loss: 0.7254 - learning_rate: 5.0000e-04\n",
      "Epoch 26/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.7845 - loss: 0.5249 - val_accuracy: 0.6803 - val_loss: 0.7373 - learning_rate: 5.0000e-04\n",
      "Epoch 27/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.7690 - loss: 0.5711 - val_accuracy: 0.6871 - val_loss: 0.7391 - learning_rate: 5.0000e-04\n",
      "Epoch 28/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.7960 - loss: 0.5252 - val_accuracy: 0.6871 - val_loss: 0.7205 - learning_rate: 5.0000e-04\n",
      "Epoch 29/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - accuracy: 0.7831 - loss: 0.5091 - val_accuracy: 0.6939 - val_loss: 0.7058 - learning_rate: 5.0000e-04\n",
      "Epoch 30/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.7716 - loss: 0.5763 - val_accuracy: 0.7075 - val_loss: 0.6907 - learning_rate: 5.0000e-04\n",
      "Epoch 31/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.8015 - loss: 0.5350 - val_accuracy: 0.6939 - val_loss: 0.7015 - learning_rate: 5.0000e-04\n",
      "Epoch 32/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.7809 - loss: 0.5210 - val_accuracy: 0.7007 - val_loss: 0.7009 - learning_rate: 5.0000e-04\n",
      "Epoch 33/100\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.7896 - loss: 0.5308\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - accuracy: 0.7921 - loss: 0.5257 - val_accuracy: 0.7007 - val_loss: 0.7165 - learning_rate: 5.0000e-04\n",
      "Epoch 34/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.8230 - loss: 0.4733 - val_accuracy: 0.7075 - val_loss: 0.7155 - learning_rate: 2.5000e-04\n",
      "Epoch 35/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8190 - loss: 0.4598 - val_accuracy: 0.7075 - val_loss: 0.7221 - learning_rate: 2.5000e-04\n",
      "Epoch 36/100\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.7962 - loss: 0.4812\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.7950 - loss: 0.4853 - val_accuracy: 0.7075 - val_loss: 0.7083 - learning_rate: 2.5000e-04\n",
      "Epoch 37/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.8048 - loss: 0.4771 - val_accuracy: 0.7075 - val_loss: 0.7035 - learning_rate: 1.2500e-04\n",
      "Epoch 38/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.7760 - loss: 0.4933 - val_accuracy: 0.7075 - val_loss: 0.7024 - learning_rate: 1.2500e-04\n",
      "Epoch 39/100\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.8293 - loss: 0.4188\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.8280 - loss: 0.4262 - val_accuracy: 0.7007 - val_loss: 0.6962 - learning_rate: 1.2500e-04\n",
      "Epoch 40/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.8064 - loss: 0.4882 - val_accuracy: 0.6939 - val_loss: 0.6924 - learning_rate: 6.2500e-05\n",
      "Epoch 41/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - accuracy: 0.8416 - loss: 0.4156 - val_accuracy: 0.7211 - val_loss: 0.6895 - learning_rate: 6.2500e-05\n",
      "Epoch 42/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8321 - loss: 0.4285 - val_accuracy: 0.7279 - val_loss: 0.6873 - learning_rate: 6.2500e-05\n",
      "Epoch 43/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.8257 - loss: 0.4119 - val_accuracy: 0.7211 - val_loss: 0.6878 - learning_rate: 6.2500e-05\n",
      "Epoch 44/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - accuracy: 0.8382 - loss: 0.3915 - val_accuracy: 0.7347 - val_loss: 0.6860 - learning_rate: 6.2500e-05\n",
      "Epoch 45/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.8116 - loss: 0.4472 - val_accuracy: 0.7279 - val_loss: 0.6842 - learning_rate: 6.2500e-05\n",
      "Epoch 46/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.8438 - loss: 0.4061 - val_accuracy: 0.7211 - val_loss: 0.6817 - learning_rate: 6.2500e-05\n",
      "Epoch 47/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - accuracy: 0.8529 - loss: 0.3861 - val_accuracy: 0.7279 - val_loss: 0.6856 - learning_rate: 6.2500e-05\n",
      "Epoch 48/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.8368 - loss: 0.4000 - val_accuracy: 0.7279 - val_loss: 0.6811 - learning_rate: 6.2500e-05\n",
      "Epoch 49/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.8235 - loss: 0.4371 - val_accuracy: 0.7347 - val_loss: 0.6792 - learning_rate: 6.2500e-05\n",
      "Epoch 50/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.8256 - loss: 0.4677 - val_accuracy: 0.7347 - val_loss: 0.6793 - learning_rate: 6.2500e-05\n",
      "Epoch 51/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - accuracy: 0.8433 - loss: 0.3861 - val_accuracy: 0.7347 - val_loss: 0.6784 - learning_rate: 6.2500e-05\n",
      "Epoch 52/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.8632 - loss: 0.3502 - val_accuracy: 0.7347 - val_loss: 0.6779 - learning_rate: 6.2500e-05\n",
      "Epoch 53/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.8275 - loss: 0.4257 - val_accuracy: 0.7347 - val_loss: 0.6777 - learning_rate: 6.2500e-05\n",
      "Epoch 54/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.7771 - loss: 0.4540 - val_accuracy: 0.7347 - val_loss: 0.6755 - learning_rate: 6.2500e-05\n",
      "Epoch 55/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.8518 - loss: 0.3949 - val_accuracy: 0.7415 - val_loss: 0.6769 - learning_rate: 6.2500e-05\n",
      "Epoch 56/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - accuracy: 0.8025 - loss: 0.4367 - val_accuracy: 0.7483 - val_loss: 0.6771 - learning_rate: 6.2500e-05\n",
      "Epoch 57/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.8470 - loss: 0.3930 - val_accuracy: 0.7551 - val_loss: 0.6746 - learning_rate: 6.2500e-05\n",
      "Epoch 58/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.8376 - loss: 0.4004 - val_accuracy: 0.7415 - val_loss: 0.6736 - learning_rate: 6.2500e-05\n",
      "Epoch 59/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.8270 - loss: 0.4077 - val_accuracy: 0.7415 - val_loss: 0.6696 - learning_rate: 6.2500e-05\n",
      "Epoch 60/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.8510 - loss: 0.3693 - val_accuracy: 0.7415 - val_loss: 0.6695 - learning_rate: 6.2500e-05\n",
      "Epoch 61/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8183 - loss: 0.4198 - val_accuracy: 0.7483 - val_loss: 0.6700 - learning_rate: 6.2500e-05\n",
      "Epoch 62/100\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.8212 - loss: 0.4179\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.8248 - loss: 0.4141 - val_accuracy: 0.7551 - val_loss: 0.6722 - learning_rate: 6.2500e-05\n",
      "Epoch 63/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.8426 - loss: 0.4247 - val_accuracy: 0.7483 - val_loss: 0.6701 - learning_rate: 3.1250e-05\n",
      "Epoch 64/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.8297 - loss: 0.3996 - val_accuracy: 0.7551 - val_loss: 0.6698 - learning_rate: 3.1250e-05\n",
      "Epoch 65/100\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.8199 - loss: 0.4266\n",
      "Epoch 65: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.8234 - loss: 0.4213 - val_accuracy: 0.7619 - val_loss: 0.6698 - learning_rate: 3.1250e-05\n",
      "Epoch 66/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.8267 - loss: 0.4190 - val_accuracy: 0.7619 - val_loss: 0.6736 - learning_rate: 1.5625e-05\n",
      "Epoch 67/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.8340 - loss: 0.3861 - val_accuracy: 0.7619 - val_loss: 0.6743 - learning_rate: 1.5625e-05\n",
      "Epoch 68/100\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.8617 - loss: 0.3744\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8601 - loss: 0.3783 - val_accuracy: 0.7619 - val_loss: 0.6743 - learning_rate: 1.5625e-05\n",
      "Epoch 69/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8390 - loss: 0.3776 - val_accuracy: 0.7619 - val_loss: 0.6769 - learning_rate: 7.8125e-06\n",
      "Epoch 70/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8619 - loss: 0.3673 - val_accuracy: 0.7619 - val_loss: 0.6768 - learning_rate: 7.8125e-06\n",
      "Epoch 71/100\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.8299 - loss: 0.4003\n",
      "Epoch 71: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8300 - loss: 0.3987 - val_accuracy: 0.7619 - val_loss: 0.6762 - learning_rate: 7.8125e-06\n",
      "Epoch 72/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - accuracy: 0.8440 - loss: 0.3839 - val_accuracy: 0.7619 - val_loss: 0.6781 - learning_rate: 3.9063e-06\n",
      "Epoch 73/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.8450 - loss: 0.3871 - val_accuracy: 0.7619 - val_loss: 0.6789 - learning_rate: 3.9063e-06\n",
      "Epoch 74/100\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.8365 - loss: 0.4196\n",
      "Epoch 74: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.8352 - loss: 0.4161 - val_accuracy: 0.7551 - val_loss: 0.6796 - learning_rate: 3.9063e-06\n",
      "Epoch 75/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.8447 - loss: 0.3716 - val_accuracy: 0.7483 - val_loss: 0.6807 - learning_rate: 1.9531e-06\n",
      "Epoch 76/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.8350 - loss: 0.3887 - val_accuracy: 0.7483 - val_loss: 0.6814 - learning_rate: 1.9531e-06\n",
      "Epoch 77/100\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.8622 - loss: 0.3778\n",
      "Epoch 77: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.8580 - loss: 0.3820 - val_accuracy: 0.7483 - val_loss: 0.6824 - learning_rate: 1.9531e-06\n",
      "Epoch 78/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8527 - loss: 0.3730 - val_accuracy: 0.7483 - val_loss: 0.6816 - learning_rate: 1.0000e-06\n",
      "Epoch 79/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.8346 - loss: 0.3918 - val_accuracy: 0.7483 - val_loss: 0.6831 - learning_rate: 1.0000e-06\n",
      "Epoch 80/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8305 - loss: 0.3582 - val_accuracy: 0.7483 - val_loss: 0.6819 - learning_rate: 1.0000e-06\n",
      "Epoch 81/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8077 - loss: 0.4474 - val_accuracy: 0.7551 - val_loss: 0.6802 - learning_rate: 1.0000e-06\n",
      "Epoch 82/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8598 - loss: 0.3662 - val_accuracy: 0.7551 - val_loss: 0.6803 - learning_rate: 1.0000e-06\n",
      "Epoch 83/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.8486 - loss: 0.4033 - val_accuracy: 0.7619 - val_loss: 0.6799 - learning_rate: 1.0000e-06\n",
      "Epoch 84/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8585 - loss: 0.3620 - val_accuracy: 0.7551 - val_loss: 0.6801 - learning_rate: 1.0000e-06\n",
      "Epoch 85/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8150 - loss: 0.4383 - val_accuracy: 0.7551 - val_loss: 0.6783 - learning_rate: 1.0000e-06\n",
      "Epoch 86/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.8447 - loss: 0.4192 - val_accuracy: 0.7551 - val_loss: 0.6782 - learning_rate: 1.0000e-06\n",
      "Epoch 87/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8251 - loss: 0.4326 - val_accuracy: 0.7551 - val_loss: 0.6780 - learning_rate: 1.0000e-06\n",
      "Epoch 88/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.8820 - loss: 0.3135 - val_accuracy: 0.7551 - val_loss: 0.6782 - learning_rate: 1.0000e-06\n",
      "Epoch 89/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - accuracy: 0.8467 - loss: 0.3595 - val_accuracy: 0.7551 - val_loss: 0.6816 - learning_rate: 1.0000e-06\n",
      "Epoch 90/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.8599 - loss: 0.3369 - val_accuracy: 0.7551 - val_loss: 0.6805 - learning_rate: 1.0000e-06\n",
      "Epoch 91/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8648 - loss: 0.3726 - val_accuracy: 0.7551 - val_loss: 0.6823 - learning_rate: 1.0000e-06\n",
      "Epoch 92/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.8593 - loss: 0.3748 - val_accuracy: 0.7551 - val_loss: 0.6807 - learning_rate: 1.0000e-06\n",
      "Epoch 93/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8397 - loss: 0.3956 - val_accuracy: 0.7551 - val_loss: 0.6800 - learning_rate: 1.0000e-06\n",
      "Epoch 94/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8336 - loss: 0.4174 - val_accuracy: 0.7619 - val_loss: 0.6788 - learning_rate: 1.0000e-06\n",
      "Epoch 95/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.8397 - loss: 0.4106 - val_accuracy: 0.7619 - val_loss: 0.6769 - learning_rate: 1.0000e-06\n",
      "Epoch 96/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.8395 - loss: 0.3849 - val_accuracy: 0.7619 - val_loss: 0.6765 - learning_rate: 1.0000e-06\n",
      "Epoch 97/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8382 - loss: 0.3822 - val_accuracy: 0.7619 - val_loss: 0.6763 - learning_rate: 1.0000e-06\n",
      "Epoch 98/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8460 - loss: 0.3976 - val_accuracy: 0.7551 - val_loss: 0.6772 - learning_rate: 1.0000e-06\n",
      "Epoch 99/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8446 - loss: 0.4024 - val_accuracy: 0.7619 - val_loss: 0.6748 - learning_rate: 1.0000e-06\n",
      "Epoch 100/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8670 - loss: 0.3598 - val_accuracy: 0.7619 - val_loss: 0.6755 - learning_rate: 1.0000e-06\n",
      "Epoch 1/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 89ms/step - accuracy: 0.3587 - loss: 1.8074 - val_accuracy: 0.6507 - val_loss: 0.9533 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.5051 - loss: 1.2955 - val_accuracy: 0.6370 - val_loss: 0.9356 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.5340 - loss: 1.2255 - val_accuracy: 0.6096 - val_loss: 0.9053 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.5595 - loss: 1.2170 - val_accuracy: 0.5959 - val_loss: 0.8921 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.5804 - loss: 1.0573 - val_accuracy: 0.6370 - val_loss: 0.8414 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.5820 - loss: 1.0517 - val_accuracy: 0.6781 - val_loss: 0.8136 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.6317 - loss: 0.8975 - val_accuracy: 0.6370 - val_loss: 0.7985 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.6221 - loss: 1.0461 - val_accuracy: 0.6027 - val_loss: 0.8306 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.6387 - loss: 0.9316 - val_accuracy: 0.5890 - val_loss: 0.8307 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.6551 - loss: 0.9536 - val_accuracy: 0.6781 - val_loss: 0.7361 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.6968 - loss: 0.7946 - val_accuracy: 0.6918 - val_loss: 0.7334 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.7152 - loss: 0.8281 - val_accuracy: 0.6301 - val_loss: 0.7663 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.6795 - loss: 0.8100 - val_accuracy: 0.6644 - val_loss: 0.7431 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.7189 - loss: 0.6970 - val_accuracy: 0.7192 - val_loss: 0.7224 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.7334 - loss: 0.6501 - val_accuracy: 0.7123 - val_loss: 0.7272 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.7037 - loss: 0.7432 - val_accuracy: 0.7055 - val_loss: 0.7069 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.7115 - loss: 0.6656 - val_accuracy: 0.7192 - val_loss: 0.7285 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.7403 - loss: 0.6428 - val_accuracy: 0.6781 - val_loss: 0.7310 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.7429 - loss: 0.7013 - val_accuracy: 0.7329 - val_loss: 0.6921 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.7530 - loss: 0.6366 - val_accuracy: 0.7397 - val_loss: 0.6497 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - accuracy: 0.7511 - loss: 0.6505 - val_accuracy: 0.7123 - val_loss: 0.6636 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.7660 - loss: 0.5641 - val_accuracy: 0.7534 - val_loss: 0.6278 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.7399 - loss: 0.6285 - val_accuracy: 0.7671 - val_loss: 0.6249 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.7657 - loss: 0.5529 - val_accuracy: 0.7740 - val_loss: 0.6194 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.7460 - loss: 0.6553 - val_accuracy: 0.7603 - val_loss: 0.6089 - learning_rate: 0.0010\n",
      "Epoch 26/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.7542 - loss: 0.5640 - val_accuracy: 0.7466 - val_loss: 0.5973 - learning_rate: 0.0010\n",
      "Epoch 27/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.7694 - loss: 0.6090 - val_accuracy: 0.7192 - val_loss: 0.6330 - learning_rate: 0.0010\n",
      "Epoch 28/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.8077 - loss: 0.5030 - val_accuracy: 0.7055 - val_loss: 0.6860 - learning_rate: 0.0010\n",
      "Epoch 29/100\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.7736 - loss: 0.5395\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.7745 - loss: 0.5385 - val_accuracy: 0.7260 - val_loss: 0.6607 - learning_rate: 0.0010\n",
      "Epoch 30/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - accuracy: 0.7844 - loss: 0.5189 - val_accuracy: 0.7397 - val_loss: 0.6281 - learning_rate: 5.0000e-04\n",
      "Epoch 31/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8209 - loss: 0.4927 - val_accuracy: 0.7397 - val_loss: 0.6267 - learning_rate: 5.0000e-04\n",
      "Epoch 32/100\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.8164 - loss: 0.4826\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.8147 - loss: 0.4824 - val_accuracy: 0.7329 - val_loss: 0.6174 - learning_rate: 5.0000e-04\n",
      "Epoch 33/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8080 - loss: 0.4642 - val_accuracy: 0.7329 - val_loss: 0.6077 - learning_rate: 2.5000e-04\n",
      "Epoch 34/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.8241 - loss: 0.4671 - val_accuracy: 0.7329 - val_loss: 0.6013 - learning_rate: 2.5000e-04\n",
      "Epoch 35/100\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.8134 - loss: 0.4583\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8151 - loss: 0.4559 - val_accuracy: 0.7397 - val_loss: 0.5981 - learning_rate: 2.5000e-04\n",
      "Epoch 36/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.8515 - loss: 0.3940 - val_accuracy: 0.7466 - val_loss: 0.6013 - learning_rate: 1.2500e-04\n",
      "Epoch 37/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.7969 - loss: 0.4608 - val_accuracy: 0.7397 - val_loss: 0.6085 - learning_rate: 1.2500e-04\n",
      "Epoch 38/100\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.8282 - loss: 0.4615\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.8337 - loss: 0.4474 - val_accuracy: 0.7397 - val_loss: 0.6140 - learning_rate: 1.2500e-04\n",
      "Epoch 39/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.7959 - loss: 0.4810 - val_accuracy: 0.7397 - val_loss: 0.6145 - learning_rate: 6.2500e-05\n",
      "Epoch 40/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.8483 - loss: 0.4042 - val_accuracy: 0.7397 - val_loss: 0.6132 - learning_rate: 6.2500e-05\n",
      "Epoch 41/100\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.8460 - loss: 0.3977\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8451 - loss: 0.3984 - val_accuracy: 0.7397 - val_loss: 0.6128 - learning_rate: 6.2500e-05\n",
      "Epoch 42/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8517 - loss: 0.3804 - val_accuracy: 0.7397 - val_loss: 0.6106 - learning_rate: 3.1250e-05\n",
      "Epoch 43/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.8340 - loss: 0.4044 - val_accuracy: 0.7329 - val_loss: 0.6114 - learning_rate: 3.1250e-05\n",
      "Epoch 44/100\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.8657 - loss: 0.3592\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8631 - loss: 0.3639 - val_accuracy: 0.7260 - val_loss: 0.6103 - learning_rate: 3.1250e-05\n",
      "Epoch 45/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.8211 - loss: 0.4166 - val_accuracy: 0.7329 - val_loss: 0.6100 - learning_rate: 1.5625e-05\n",
      "Epoch 46/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - accuracy: 0.8485 - loss: 0.3895 - val_accuracy: 0.7260 - val_loss: 0.6125 - learning_rate: 1.5625e-05\n",
      "Epoch 47/100\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.8471 - loss: 0.3752\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8470 - loss: 0.3769 - val_accuracy: 0.7260 - val_loss: 0.6124 - learning_rate: 1.5625e-05\n",
      "Epoch 48/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.8196 - loss: 0.4140 - val_accuracy: 0.7260 - val_loss: 0.6124 - learning_rate: 7.8125e-06\n",
      "Epoch 49/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8743 - loss: 0.3709 - val_accuracy: 0.7192 - val_loss: 0.6125 - learning_rate: 7.8125e-06\n",
      "Epoch 50/100\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.8487 - loss: 0.4149\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8473 - loss: 0.4163 - val_accuracy: 0.7192 - val_loss: 0.6110 - learning_rate: 7.8125e-06\n",
      "Epoch 51/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.8315 - loss: 0.4049 - val_accuracy: 0.7192 - val_loss: 0.6112 - learning_rate: 3.9063e-06\n",
      "Epoch 52/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.8483 - loss: 0.3923 - val_accuracy: 0.7192 - val_loss: 0.6116 - learning_rate: 3.9063e-06\n",
      "Epoch 53/100\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.8328 - loss: 0.3990\n",
      "Epoch 53: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8343 - loss: 0.3972 - val_accuracy: 0.7260 - val_loss: 0.6101 - learning_rate: 3.9063e-06\n",
      "Epoch 54/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.8504 - loss: 0.3841 - val_accuracy: 0.7192 - val_loss: 0.6110 - learning_rate: 1.9531e-06\n",
      "Epoch 55/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.8511 - loss: 0.4025 - val_accuracy: 0.7329 - val_loss: 0.6110 - learning_rate: 1.9531e-06\n",
      "Epoch 56/100\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.8343 - loss: 0.4058\n",
      "Epoch 56: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8334 - loss: 0.4081 - val_accuracy: 0.7329 - val_loss: 0.6107 - learning_rate: 1.9531e-06\n",
      "Epoch 57/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.8611 - loss: 0.3752 - val_accuracy: 0.7397 - val_loss: 0.6119 - learning_rate: 1.0000e-06\n",
      "Epoch 58/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8478 - loss: 0.4149 - val_accuracy: 0.7329 - val_loss: 0.6129 - learning_rate: 1.0000e-06\n",
      "Epoch 59/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.8366 - loss: 0.4501 - val_accuracy: 0.7397 - val_loss: 0.6118 - learning_rate: 1.0000e-06\n",
      "Epoch 60/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8348 - loss: 0.3887 - val_accuracy: 0.7466 - val_loss: 0.6108 - learning_rate: 1.0000e-06\n",
      "Epoch 61/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.8414 - loss: 0.3956 - val_accuracy: 0.7397 - val_loss: 0.6114 - learning_rate: 1.0000e-06\n",
      "Epoch 62/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.8361 - loss: 0.4088 - val_accuracy: 0.7534 - val_loss: 0.6097 - learning_rate: 1.0000e-06\n",
      "Epoch 63/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.8562 - loss: 0.4125 - val_accuracy: 0.7534 - val_loss: 0.6106 - learning_rate: 1.0000e-06\n",
      "Epoch 64/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.8648 - loss: 0.3551 - val_accuracy: 0.7466 - val_loss: 0.6114 - learning_rate: 1.0000e-06\n",
      "Epoch 65/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.7684 - loss: 0.4681 - val_accuracy: 0.7466 - val_loss: 0.6129 - learning_rate: 1.0000e-06\n",
      "Epoch 66/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.8224 - loss: 0.4262 - val_accuracy: 0.7534 - val_loss: 0.6116 - learning_rate: 1.0000e-06\n",
      "Epoch 67/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.8539 - loss: 0.3743 - val_accuracy: 0.7534 - val_loss: 0.6113 - learning_rate: 1.0000e-06\n",
      "Epoch 68/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.8372 - loss: 0.4186 - val_accuracy: 0.7534 - val_loss: 0.6136 - learning_rate: 1.0000e-06\n",
      "Epoch 69/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8690 - loss: 0.3370 - val_accuracy: 0.7534 - val_loss: 0.6137 - learning_rate: 1.0000e-06\n",
      "Epoch 70/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - accuracy: 0.8304 - loss: 0.4220 - val_accuracy: 0.7534 - val_loss: 0.6138 - learning_rate: 1.0000e-06\n",
      "Epoch 71/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.8315 - loss: 0.4382 - val_accuracy: 0.7534 - val_loss: 0.6149 - learning_rate: 1.0000e-06\n",
      "Epoch 72/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.8517 - loss: 0.4083 - val_accuracy: 0.7534 - val_loss: 0.6146 - learning_rate: 1.0000e-06\n",
      "Epoch 73/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.8394 - loss: 0.4042 - val_accuracy: 0.7534 - val_loss: 0.6147 - learning_rate: 1.0000e-06\n",
      "Epoch 74/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8451 - loss: 0.4282 - val_accuracy: 0.7534 - val_loss: 0.6143 - learning_rate: 1.0000e-06\n",
      "Epoch 75/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.8182 - loss: 0.4263 - val_accuracy: 0.7534 - val_loss: 0.6138 - learning_rate: 1.0000e-06\n",
      "Epoch 76/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8474 - loss: 0.3825 - val_accuracy: 0.7534 - val_loss: 0.6144 - learning_rate: 1.0000e-06\n",
      "Epoch 77/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.8007 - loss: 0.4662 - val_accuracy: 0.7534 - val_loss: 0.6163 - learning_rate: 1.0000e-06\n",
      "Epoch 78/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.8519 - loss: 0.3805 - val_accuracy: 0.7534 - val_loss: 0.6166 - learning_rate: 1.0000e-06\n",
      "Epoch 79/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.8641 - loss: 0.3975 - val_accuracy: 0.7534 - val_loss: 0.6168 - learning_rate: 1.0000e-06\n",
      "Epoch 80/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.8569 - loss: 0.4108 - val_accuracy: 0.7534 - val_loss: 0.6160 - learning_rate: 1.0000e-06\n",
      "Epoch 81/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.8301 - loss: 0.4007 - val_accuracy: 0.7534 - val_loss: 0.6174 - learning_rate: 1.0000e-06\n",
      "Epoch 82/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.8414 - loss: 0.4148 - val_accuracy: 0.7534 - val_loss: 0.6167 - learning_rate: 1.0000e-06\n",
      "Epoch 83/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8600 - loss: 0.3834 - val_accuracy: 0.7534 - val_loss: 0.6177 - learning_rate: 1.0000e-06\n",
      "Epoch 84/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8194 - loss: 0.4396 - val_accuracy: 0.7534 - val_loss: 0.6165 - learning_rate: 1.0000e-06\n",
      "Epoch 85/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.8026 - loss: 0.4812 - val_accuracy: 0.7534 - val_loss: 0.6158 - learning_rate: 1.0000e-06\n",
      "Epoch 86/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8117 - loss: 0.4550 - val_accuracy: 0.7534 - val_loss: 0.6174 - learning_rate: 1.0000e-06\n",
      "Epoch 87/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8469 - loss: 0.3905 - val_accuracy: 0.7534 - val_loss: 0.6168 - learning_rate: 1.0000e-06\n",
      "Epoch 88/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.8511 - loss: 0.3726 - val_accuracy: 0.7534 - val_loss: 0.6167 - learning_rate: 1.0000e-06\n",
      "Epoch 89/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.8293 - loss: 0.4088 - val_accuracy: 0.7603 - val_loss: 0.6168 - learning_rate: 1.0000e-06\n",
      "Epoch 90/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - accuracy: 0.8726 - loss: 0.3669 - val_accuracy: 0.7534 - val_loss: 0.6168 - learning_rate: 1.0000e-06\n",
      "Epoch 91/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8513 - loss: 0.3850 - val_accuracy: 0.7534 - val_loss: 0.6165 - learning_rate: 1.0000e-06\n",
      "Epoch 92/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.8424 - loss: 0.4050 - val_accuracy: 0.7534 - val_loss: 0.6170 - learning_rate: 1.0000e-06\n",
      "Epoch 93/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8287 - loss: 0.4336 - val_accuracy: 0.7534 - val_loss: 0.6175 - learning_rate: 1.0000e-06\n",
      "Epoch 94/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.7989 - loss: 0.4894 - val_accuracy: 0.7534 - val_loss: 0.6173 - learning_rate: 1.0000e-06\n",
      "Epoch 95/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.8515 - loss: 0.3913 - val_accuracy: 0.7603 - val_loss: 0.6175 - learning_rate: 1.0000e-06\n",
      "Epoch 96/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - accuracy: 0.8291 - loss: 0.4306 - val_accuracy: 0.7603 - val_loss: 0.6182 - learning_rate: 1.0000e-06\n",
      "Epoch 97/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.8263 - loss: 0.4279 - val_accuracy: 0.7603 - val_loss: 0.6169 - learning_rate: 1.0000e-06\n",
      "Epoch 98/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - accuracy: 0.8325 - loss: 0.4221 - val_accuracy: 0.7603 - val_loss: 0.6169 - learning_rate: 1.0000e-06\n",
      "Epoch 99/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.8162 - loss: 0.4379 - val_accuracy: 0.7603 - val_loss: 0.6171 - learning_rate: 1.0000e-06\n",
      "Epoch 100/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.8327 - loss: 0.4481 - val_accuracy: 0.7603 - val_loss: 0.6155 - learning_rate: 1.0000e-06\n",
      "Epoch 1/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 92ms/step - accuracy: 0.4124 - loss: 1.5863 - val_accuracy: 0.6233 - val_loss: 0.9978 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.5409 - loss: 1.2940 - val_accuracy: 0.6164 - val_loss: 0.9290 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.5076 - loss: 1.2315 - val_accuracy: 0.6301 - val_loss: 0.9063 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.6078 - loss: 1.0454 - val_accuracy: 0.6164 - val_loss: 0.9455 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.6079 - loss: 1.0900 - val_accuracy: 0.5685 - val_loss: 1.0452 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.6322 - loss: 0.9380\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.6311 - loss: 0.9370 - val_accuracy: 0.5616 - val_loss: 1.0118 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.6673 - loss: 0.9328 - val_accuracy: 0.5616 - val_loss: 0.9834 - learning_rate: 5.0000e-04\n",
      "Epoch 8/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.6682 - loss: 0.8533 - val_accuracy: 0.5822 - val_loss: 0.9598 - learning_rate: 5.0000e-04\n",
      "Epoch 9/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.6549 - loss: 0.7975 - val_accuracy: 0.6027 - val_loss: 0.8844 - learning_rate: 5.0000e-04\n",
      "Epoch 10/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.6776 - loss: 0.7665 - val_accuracy: 0.6301 - val_loss: 0.8490 - learning_rate: 5.0000e-04\n",
      "Epoch 11/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.6911 - loss: 0.8189 - val_accuracy: 0.6370 - val_loss: 0.8411 - learning_rate: 5.0000e-04\n",
      "Epoch 12/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.6755 - loss: 0.7892 - val_accuracy: 0.5959 - val_loss: 0.8551 - learning_rate: 5.0000e-04\n",
      "Epoch 13/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.6910 - loss: 0.7589 - val_accuracy: 0.6164 - val_loss: 0.8351 - learning_rate: 5.0000e-04\n",
      "Epoch 14/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.6774 - loss: 0.7254 - val_accuracy: 0.6096 - val_loss: 0.8195 - learning_rate: 5.0000e-04\n",
      "Epoch 15/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.7236 - loss: 0.7324 - val_accuracy: 0.6164 - val_loss: 0.8061 - learning_rate: 5.0000e-04\n",
      "Epoch 16/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.7503 - loss: 0.6181 - val_accuracy: 0.6233 - val_loss: 0.8034 - learning_rate: 5.0000e-04\n",
      "Epoch 17/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.7557 - loss: 0.6218 - val_accuracy: 0.6301 - val_loss: 0.7971 - learning_rate: 5.0000e-04\n",
      "Epoch 18/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - accuracy: 0.7327 - loss: 0.6472 - val_accuracy: 0.6507 - val_loss: 0.7800 - learning_rate: 5.0000e-04\n",
      "Epoch 19/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - accuracy: 0.6999 - loss: 0.7536 - val_accuracy: 0.6438 - val_loss: 0.7647 - learning_rate: 5.0000e-04\n",
      "Epoch 20/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.7620 - loss: 0.5725 - val_accuracy: 0.6438 - val_loss: 0.7571 - learning_rate: 5.0000e-04\n",
      "Epoch 21/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - accuracy: 0.7519 - loss: 0.6446 - val_accuracy: 0.6712 - val_loss: 0.7657 - learning_rate: 5.0000e-04\n",
      "Epoch 22/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.7682 - loss: 0.5805 - val_accuracy: 0.6986 - val_loss: 0.7635 - learning_rate: 5.0000e-04\n",
      "Epoch 23/100\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.7400 - loss: 0.5718\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.7383 - loss: 0.5804 - val_accuracy: 0.6918 - val_loss: 0.7592 - learning_rate: 5.0000e-04\n",
      "Epoch 24/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.7484 - loss: 0.5490 - val_accuracy: 0.6918 - val_loss: 0.7543 - learning_rate: 2.5000e-04\n",
      "Epoch 25/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.7856 - loss: 0.5777 - val_accuracy: 0.6781 - val_loss: 0.7414 - learning_rate: 2.5000e-04\n",
      "Epoch 26/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.7930 - loss: 0.5293 - val_accuracy: 0.6781 - val_loss: 0.7332 - learning_rate: 2.5000e-04\n",
      "Epoch 27/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.7950 - loss: 0.5859 - val_accuracy: 0.6849 - val_loss: 0.7296 - learning_rate: 2.5000e-04\n",
      "Epoch 28/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.8029 - loss: 0.5303 - val_accuracy: 0.6781 - val_loss: 0.7376 - learning_rate: 2.5000e-04\n",
      "Epoch 29/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.7557 - loss: 0.5826 - val_accuracy: 0.6849 - val_loss: 0.7503 - learning_rate: 2.5000e-04\n",
      "Epoch 30/100\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.7899 - loss: 0.5225\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.7905 - loss: 0.5213 - val_accuracy: 0.6644 - val_loss: 0.7601 - learning_rate: 2.5000e-04\n",
      "Epoch 31/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.7809 - loss: 0.5613 - val_accuracy: 0.6712 - val_loss: 0.7545 - learning_rate: 1.2500e-04\n",
      "Epoch 32/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.8147 - loss: 0.4946 - val_accuracy: 0.6781 - val_loss: 0.7507 - learning_rate: 1.2500e-04\n",
      "Epoch 33/100\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.8071 - loss: 0.5146\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.8081 - loss: 0.5148 - val_accuracy: 0.6781 - val_loss: 0.7572 - learning_rate: 1.2500e-04\n",
      "Epoch 34/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - accuracy: 0.8031 - loss: 0.4717 - val_accuracy: 0.6781 - val_loss: 0.7617 - learning_rate: 6.2500e-05\n",
      "Epoch 35/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - accuracy: 0.7900 - loss: 0.5349 - val_accuracy: 0.6849 - val_loss: 0.7640 - learning_rate: 6.2500e-05\n",
      "Epoch 36/100\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.7748 - loss: 0.5761\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.7776 - loss: 0.5668 - val_accuracy: 0.6849 - val_loss: 0.7615 - learning_rate: 6.2500e-05\n",
      "Epoch 37/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8287 - loss: 0.4140 - val_accuracy: 0.6849 - val_loss: 0.7612 - learning_rate: 3.1250e-05\n",
      "Epoch 38/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8210 - loss: 0.4528 - val_accuracy: 0.6849 - val_loss: 0.7622 - learning_rate: 3.1250e-05\n",
      "Epoch 39/100\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.7948 - loss: 0.5104\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.7983 - loss: 0.5071 - val_accuracy: 0.6849 - val_loss: 0.7629 - learning_rate: 3.1250e-05\n",
      "Epoch 40/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.8106 - loss: 0.4544 - val_accuracy: 0.6849 - val_loss: 0.7647 - learning_rate: 1.5625e-05\n",
      "Epoch 41/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - accuracy: 0.7808 - loss: 0.5582 - val_accuracy: 0.6849 - val_loss: 0.7670 - learning_rate: 1.5625e-05\n",
      "Epoch 42/100\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.7812 - loss: 0.5461\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - accuracy: 0.7822 - loss: 0.5460 - val_accuracy: 0.6849 - val_loss: 0.7694 - learning_rate: 1.5625e-05\n",
      "Epoch 43/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.7942 - loss: 0.5143 - val_accuracy: 0.6781 - val_loss: 0.7735 - learning_rate: 7.8125e-06\n",
      "Epoch 44/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.7801 - loss: 0.5539 - val_accuracy: 0.6781 - val_loss: 0.7758 - learning_rate: 7.8125e-06\n",
      "Epoch 45/100\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.8190 - loss: 0.4974\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8166 - loss: 0.4984 - val_accuracy: 0.6849 - val_loss: 0.7796 - learning_rate: 7.8125e-06\n",
      "Epoch 46/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8150 - loss: 0.4614 - val_accuracy: 0.6849 - val_loss: 0.7822 - learning_rate: 3.9063e-06\n",
      "Epoch 47/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.8029 - loss: 0.4353 - val_accuracy: 0.6849 - val_loss: 0.7846 - learning_rate: 3.9063e-06\n",
      "Epoch 48/100\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.8189 - loss: 0.4565\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8131 - loss: 0.4687 - val_accuracy: 0.6849 - val_loss: 0.7867 - learning_rate: 3.9063e-06\n",
      "Epoch 49/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.8361 - loss: 0.4298 - val_accuracy: 0.6781 - val_loss: 0.7895 - learning_rate: 1.9531e-06\n",
      "Epoch 50/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - accuracy: 0.8087 - loss: 0.4891 - val_accuracy: 0.6781 - val_loss: 0.7912 - learning_rate: 1.9531e-06\n",
      "Epoch 51/100\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.8010 - loss: 0.5547\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.8015 - loss: 0.5480 - val_accuracy: 0.6781 - val_loss: 0.7929 - learning_rate: 1.9531e-06\n",
      "Epoch 52/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.8028 - loss: 0.4828 - val_accuracy: 0.6781 - val_loss: 0.7965 - learning_rate: 1.0000e-06\n",
      "Epoch 53/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - accuracy: 0.7971 - loss: 0.5230 - val_accuracy: 0.6781 - val_loss: 0.7984 - learning_rate: 1.0000e-06\n",
      "Epoch 54/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8245 - loss: 0.4963 - val_accuracy: 0.6781 - val_loss: 0.8008 - learning_rate: 1.0000e-06\n",
      "Epoch 55/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.7798 - loss: 0.5178 - val_accuracy: 0.6781 - val_loss: 0.8018 - learning_rate: 1.0000e-06\n",
      "Epoch 56/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.8006 - loss: 0.4927 - val_accuracy: 0.6781 - val_loss: 0.8038 - learning_rate: 1.0000e-06\n",
      "Epoch 57/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.8242 - loss: 0.5060 - val_accuracy: 0.6781 - val_loss: 0.8066 - learning_rate: 1.0000e-06\n",
      "Epoch 58/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.7934 - loss: 0.5570 - val_accuracy: 0.6781 - val_loss: 0.8088 - learning_rate: 1.0000e-06\n",
      "Epoch 59/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.8093 - loss: 0.4580 - val_accuracy: 0.6781 - val_loss: 0.8123 - learning_rate: 1.0000e-06\n",
      "Epoch 60/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.8003 - loss: 0.4997 - val_accuracy: 0.6781 - val_loss: 0.8152 - learning_rate: 1.0000e-06\n",
      "Epoch 61/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.8107 - loss: 0.4965 - val_accuracy: 0.6781 - val_loss: 0.8153 - learning_rate: 1.0000e-06\n",
      "Epoch 62/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.7908 - loss: 0.5072 - val_accuracy: 0.6781 - val_loss: 0.8170 - learning_rate: 1.0000e-06\n",
      "Epoch 63/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.7967 - loss: 0.5062 - val_accuracy: 0.6781 - val_loss: 0.8208 - learning_rate: 1.0000e-06\n",
      "Epoch 64/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.8167 - loss: 0.4468 - val_accuracy: 0.6781 - val_loss: 0.8236 - learning_rate: 1.0000e-06\n",
      "Epoch 65/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.7931 - loss: 0.5053 - val_accuracy: 0.6781 - val_loss: 0.8255 - learning_rate: 1.0000e-06\n",
      "Epoch 66/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.7861 - loss: 0.5174 - val_accuracy: 0.6781 - val_loss: 0.8268 - learning_rate: 1.0000e-06\n",
      "Epoch 67/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - accuracy: 0.8147 - loss: 0.4540 - val_accuracy: 0.6781 - val_loss: 0.8277 - learning_rate: 1.0000e-06\n",
      "Epoch 68/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8053 - loss: 0.5027 - val_accuracy: 0.6781 - val_loss: 0.8298 - learning_rate: 1.0000e-06\n",
      "Epoch 69/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.7814 - loss: 0.5214 - val_accuracy: 0.6781 - val_loss: 0.8305 - learning_rate: 1.0000e-06\n",
      "Epoch 70/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.8255 - loss: 0.4618 - val_accuracy: 0.6781 - val_loss: 0.8305 - learning_rate: 1.0000e-06\n",
      "Epoch 71/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.7927 - loss: 0.5335 - val_accuracy: 0.6849 - val_loss: 0.8330 - learning_rate: 1.0000e-06\n",
      "Epoch 72/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.8010 - loss: 0.5046 - val_accuracy: 0.6849 - val_loss: 0.8334 - learning_rate: 1.0000e-06\n",
      "Epoch 73/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.7902 - loss: 0.5088 - val_accuracy: 0.6849 - val_loss: 0.8363 - learning_rate: 1.0000e-06\n",
      "Epoch 74/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.7974 - loss: 0.4526 - val_accuracy: 0.6849 - val_loss: 0.8374 - learning_rate: 1.0000e-06\n",
      "Epoch 75/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.8263 - loss: 0.4469 - val_accuracy: 0.6781 - val_loss: 0.8368 - learning_rate: 1.0000e-06\n",
      "Epoch 76/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.8199 - loss: 0.4776 - val_accuracy: 0.6849 - val_loss: 0.8385 - learning_rate: 1.0000e-06\n",
      "Epoch 77/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.8157 - loss: 0.4673 - val_accuracy: 0.6849 - val_loss: 0.8394 - learning_rate: 1.0000e-06\n",
      "Epoch 78/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.7843 - loss: 0.5514 - val_accuracy: 0.6849 - val_loss: 0.8418 - learning_rate: 1.0000e-06\n",
      "Epoch 79/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8043 - loss: 0.4894 - val_accuracy: 0.6849 - val_loss: 0.8435 - learning_rate: 1.0000e-06\n",
      "Epoch 80/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.8119 - loss: 0.4809 - val_accuracy: 0.6781 - val_loss: 0.8412 - learning_rate: 1.0000e-06\n",
      "Epoch 81/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.8262 - loss: 0.4475 - val_accuracy: 0.6781 - val_loss: 0.8418 - learning_rate: 1.0000e-06\n",
      "Epoch 82/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8008 - loss: 0.4862 - val_accuracy: 0.6781 - val_loss: 0.8427 - learning_rate: 1.0000e-06\n",
      "Epoch 83/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.7858 - loss: 0.4959 - val_accuracy: 0.6781 - val_loss: 0.8436 - learning_rate: 1.0000e-06\n",
      "Epoch 84/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - accuracy: 0.8189 - loss: 0.4747 - val_accuracy: 0.6849 - val_loss: 0.8438 - learning_rate: 1.0000e-06\n",
      "Epoch 85/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.8162 - loss: 0.4558 - val_accuracy: 0.6781 - val_loss: 0.8433 - learning_rate: 1.0000e-06\n",
      "Epoch 86/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.7870 - loss: 0.5003 - val_accuracy: 0.6849 - val_loss: 0.8433 - learning_rate: 1.0000e-06\n",
      "Epoch 87/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.8294 - loss: 0.4609 - val_accuracy: 0.6849 - val_loss: 0.8456 - learning_rate: 1.0000e-06\n",
      "Epoch 88/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.8359 - loss: 0.4386 - val_accuracy: 0.6849 - val_loss: 0.8464 - learning_rate: 1.0000e-06\n",
      "Epoch 89/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8148 - loss: 0.5160 - val_accuracy: 0.6849 - val_loss: 0.8463 - learning_rate: 1.0000e-06\n",
      "Epoch 90/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.7933 - loss: 0.5516 - val_accuracy: 0.6849 - val_loss: 0.8469 - learning_rate: 1.0000e-06\n",
      "Epoch 91/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.7900 - loss: 0.5211 - val_accuracy: 0.6781 - val_loss: 0.8456 - learning_rate: 1.0000e-06\n",
      "Epoch 92/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.8039 - loss: 0.4702 - val_accuracy: 0.6781 - val_loss: 0.8463 - learning_rate: 1.0000e-06\n",
      "Epoch 93/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.8187 - loss: 0.4265 - val_accuracy: 0.6781 - val_loss: 0.8471 - learning_rate: 1.0000e-06\n",
      "Epoch 94/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.8222 - loss: 0.4718 - val_accuracy: 0.6712 - val_loss: 0.8467 - learning_rate: 1.0000e-06\n",
      "Epoch 95/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - accuracy: 0.7954 - loss: 0.4997 - val_accuracy: 0.6781 - val_loss: 0.8464 - learning_rate: 1.0000e-06\n",
      "Epoch 96/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.7906 - loss: 0.5442 - val_accuracy: 0.6781 - val_loss: 0.8462 - learning_rate: 1.0000e-06\n",
      "Epoch 97/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.7781 - loss: 0.6039 - val_accuracy: 0.6781 - val_loss: 0.8451 - learning_rate: 1.0000e-06\n",
      "Epoch 98/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.7947 - loss: 0.5131 - val_accuracy: 0.6781 - val_loss: 0.8449 - learning_rate: 1.0000e-06\n",
      "Epoch 99/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8162 - loss: 0.4610 - val_accuracy: 0.6781 - val_loss: 0.8438 - learning_rate: 1.0000e-06\n",
      "Epoch 100/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.8264 - loss: 0.4563 - val_accuracy: 0.6781 - val_loss: 0.8416 - learning_rate: 1.0000e-06\n",
      "Epoch 1/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 91ms/step - accuracy: 0.3868 - loss: 1.7798 - val_accuracy: 0.6233 - val_loss: 1.0115 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.5294 - loss: 1.2346 - val_accuracy: 0.6575 - val_loss: 0.9280 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.5928 - loss: 1.0792 - val_accuracy: 0.6096 - val_loss: 0.8976 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.6429 - loss: 1.0294 - val_accuracy: 0.6164 - val_loss: 0.8734 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.6467 - loss: 0.9295 - val_accuracy: 0.5685 - val_loss: 0.9022 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.6256 - loss: 0.9492 - val_accuracy: 0.6233 - val_loss: 0.8522 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.7125 - loss: 0.8123 - val_accuracy: 0.5959 - val_loss: 0.8514 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - accuracy: 0.6757 - loss: 0.8191 - val_accuracy: 0.6507 - val_loss: 0.8263 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.7071 - loss: 0.7845 - val_accuracy: 0.6438 - val_loss: 0.8128 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.6779 - loss: 0.8288 - val_accuracy: 0.6712 - val_loss: 0.7788 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.7070 - loss: 0.7906 - val_accuracy: 0.6849 - val_loss: 0.7728 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.7212 - loss: 0.7281 - val_accuracy: 0.6781 - val_loss: 0.7636 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.7320 - loss: 0.6912 - val_accuracy: 0.6575 - val_loss: 0.7448 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.7442 - loss: 0.7075 - val_accuracy: 0.6644 - val_loss: 0.7651 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.7103 - loss: 0.7203 - val_accuracy: 0.6507 - val_loss: 0.7598 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.6857 - loss: 0.7449 - val_accuracy: 0.6781 - val_loss: 0.7213 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.7204 - loss: 0.6712 - val_accuracy: 0.6233 - val_loss: 0.7311 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.7560 - loss: 0.6208 - val_accuracy: 0.6164 - val_loss: 0.7841 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.7588 - loss: 0.6469\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.7583 - loss: 0.6457 - val_accuracy: 0.6370 - val_loss: 0.7472 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.7274 - loss: 0.6437 - val_accuracy: 0.6438 - val_loss: 0.7405 - learning_rate: 5.0000e-04\n",
      "Epoch 21/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.7741 - loss: 0.5653 - val_accuracy: 0.6438 - val_loss: 0.7306 - learning_rate: 5.0000e-04\n",
      "Epoch 22/100\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.7266 - loss: 0.6338\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.7288 - loss: 0.6299 - val_accuracy: 0.6507 - val_loss: 0.7307 - learning_rate: 5.0000e-04\n",
      "Epoch 23/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.7623 - loss: 0.5867 - val_accuracy: 0.6507 - val_loss: 0.7318 - learning_rate: 2.5000e-04\n",
      "Epoch 24/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.7652 - loss: 0.5554 - val_accuracy: 0.6507 - val_loss: 0.7367 - learning_rate: 2.5000e-04\n",
      "Epoch 25/100\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.7811 - loss: 0.5286\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - accuracy: 0.7790 - loss: 0.5353 - val_accuracy: 0.6849 - val_loss: 0.7334 - learning_rate: 2.5000e-04\n",
      "Epoch 26/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.8061 - loss: 0.5179 - val_accuracy: 0.6644 - val_loss: 0.7262 - learning_rate: 1.2500e-04\n",
      "Epoch 27/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.7848 - loss: 0.5282 - val_accuracy: 0.6507 - val_loss: 0.7193 - learning_rate: 1.2500e-04\n",
      "Epoch 28/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.8133 - loss: 0.4983 - val_accuracy: 0.6507 - val_loss: 0.7169 - learning_rate: 1.2500e-04\n",
      "Epoch 29/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.7972 - loss: 0.4763 - val_accuracy: 0.6712 - val_loss: 0.7144 - learning_rate: 1.2500e-04\n",
      "Epoch 30/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.8018 - loss: 0.5088 - val_accuracy: 0.6781 - val_loss: 0.7116 - learning_rate: 1.2500e-04\n",
      "Epoch 31/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.7968 - loss: 0.5092 - val_accuracy: 0.6849 - val_loss: 0.7091 - learning_rate: 1.2500e-04\n",
      "Epoch 32/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.8002 - loss: 0.5202 - val_accuracy: 0.6918 - val_loss: 0.7091 - learning_rate: 1.2500e-04\n",
      "Epoch 33/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.7933 - loss: 0.4888 - val_accuracy: 0.6918 - val_loss: 0.7101 - learning_rate: 1.2500e-04\n",
      "Epoch 34/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.7979 - loss: 0.4912 - val_accuracy: 0.6918 - val_loss: 0.7019 - learning_rate: 1.2500e-04\n",
      "Epoch 35/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.8048 - loss: 0.4686 - val_accuracy: 0.6918 - val_loss: 0.6998 - learning_rate: 1.2500e-04\n",
      "Epoch 36/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.7823 - loss: 0.5289 - val_accuracy: 0.6918 - val_loss: 0.7032 - learning_rate: 1.2500e-04\n",
      "Epoch 37/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.8499 - loss: 0.4142 - val_accuracy: 0.6918 - val_loss: 0.7024 - learning_rate: 1.2500e-04\n",
      "Epoch 38/100\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.7980 - loss: 0.5240\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - accuracy: 0.7991 - loss: 0.5210 - val_accuracy: 0.6918 - val_loss: 0.7034 - learning_rate: 1.2500e-04\n",
      "Epoch 39/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.8159 - loss: 0.4520 - val_accuracy: 0.6849 - val_loss: 0.7046 - learning_rate: 6.2500e-05\n",
      "Epoch 40/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - accuracy: 0.8018 - loss: 0.4979 - val_accuracy: 0.6849 - val_loss: 0.7051 - learning_rate: 6.2500e-05\n",
      "Epoch 41/100\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.8222 - loss: 0.4613\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.8213 - loss: 0.4617 - val_accuracy: 0.6849 - val_loss: 0.7097 - learning_rate: 6.2500e-05\n",
      "Epoch 42/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.7721 - loss: 0.5349 - val_accuracy: 0.6781 - val_loss: 0.7066 - learning_rate: 3.1250e-05\n",
      "Epoch 43/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.7846 - loss: 0.5011 - val_accuracy: 0.6849 - val_loss: 0.7056 - learning_rate: 3.1250e-05\n",
      "Epoch 44/100\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.8017 - loss: 0.4750\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.8036 - loss: 0.4717 - val_accuracy: 0.6781 - val_loss: 0.7033 - learning_rate: 3.1250e-05\n",
      "Epoch 45/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.8067 - loss: 0.4615 - val_accuracy: 0.6918 - val_loss: 0.7001 - learning_rate: 1.5625e-05\n",
      "Epoch 46/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8029 - loss: 0.4639 - val_accuracy: 0.6986 - val_loss: 0.6995 - learning_rate: 1.5625e-05\n",
      "Epoch 47/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.8070 - loss: 0.4672 - val_accuracy: 0.7055 - val_loss: 0.6990 - learning_rate: 1.5625e-05\n",
      "Epoch 48/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.8132 - loss: 0.4472 - val_accuracy: 0.7055 - val_loss: 0.6981 - learning_rate: 1.5625e-05\n",
      "Epoch 49/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.7828 - loss: 0.5029 - val_accuracy: 0.7055 - val_loss: 0.6945 - learning_rate: 1.5625e-05\n",
      "Epoch 50/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.8142 - loss: 0.4701 - val_accuracy: 0.7055 - val_loss: 0.6932 - learning_rate: 1.5625e-05\n",
      "Epoch 51/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.8053 - loss: 0.4733 - val_accuracy: 0.7123 - val_loss: 0.6931 - learning_rate: 1.5625e-05\n",
      "Epoch 52/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.8099 - loss: 0.4731 - val_accuracy: 0.7192 - val_loss: 0.6929 - learning_rate: 1.5625e-05\n",
      "Epoch 53/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8164 - loss: 0.4696 - val_accuracy: 0.7192 - val_loss: 0.6924 - learning_rate: 1.5625e-05\n",
      "Epoch 54/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.8176 - loss: 0.4314 - val_accuracy: 0.7192 - val_loss: 0.6916 - learning_rate: 1.5625e-05\n",
      "Epoch 55/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.7832 - loss: 0.4508 - val_accuracy: 0.7123 - val_loss: 0.6923 - learning_rate: 1.5625e-05\n",
      "Epoch 56/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.8090 - loss: 0.4686 - val_accuracy: 0.7123 - val_loss: 0.6919 - learning_rate: 1.5625e-05\n",
      "Epoch 57/100\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.8181 - loss: 0.4496\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.8164 - loss: 0.4538 - val_accuracy: 0.7123 - val_loss: 0.6921 - learning_rate: 1.5625e-05\n",
      "Epoch 58/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.7905 - loss: 0.4877 - val_accuracy: 0.7123 - val_loss: 0.6920 - learning_rate: 7.8125e-06\n",
      "Epoch 59/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8024 - loss: 0.4597 - val_accuracy: 0.7123 - val_loss: 0.6934 - learning_rate: 7.8125e-06\n",
      "Epoch 60/100\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.8144 - loss: 0.4220\n",
      "Epoch 60: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.8125 - loss: 0.4260 - val_accuracy: 0.7123 - val_loss: 0.6933 - learning_rate: 7.8125e-06\n",
      "Epoch 61/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8494 - loss: 0.3899 - val_accuracy: 0.7123 - val_loss: 0.6941 - learning_rate: 3.9063e-06\n",
      "Epoch 62/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.8181 - loss: 0.4562 - val_accuracy: 0.7123 - val_loss: 0.6955 - learning_rate: 3.9063e-06\n",
      "Epoch 63/100\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.8123 - loss: 0.4568\n",
      "Epoch 63: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.8148 - loss: 0.4537 - val_accuracy: 0.7123 - val_loss: 0.6960 - learning_rate: 3.9063e-06\n",
      "Epoch 64/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.8224 - loss: 0.4496 - val_accuracy: 0.7123 - val_loss: 0.6965 - learning_rate: 1.9531e-06\n",
      "Epoch 65/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.8137 - loss: 0.4367 - val_accuracy: 0.7123 - val_loss: 0.6970 - learning_rate: 1.9531e-06\n",
      "Epoch 66/100\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.8159 - loss: 0.4509\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.8137 - loss: 0.4545 - val_accuracy: 0.7123 - val_loss: 0.6976 - learning_rate: 1.9531e-06\n",
      "Epoch 67/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.8078 - loss: 0.4264 - val_accuracy: 0.7123 - val_loss: 0.6982 - learning_rate: 1.0000e-06\n",
      "Epoch 68/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.8257 - loss: 0.4187 - val_accuracy: 0.7123 - val_loss: 0.6981 - learning_rate: 1.0000e-06\n",
      "Epoch 69/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.7927 - loss: 0.5189 - val_accuracy: 0.7123 - val_loss: 0.6989 - learning_rate: 1.0000e-06\n",
      "Epoch 70/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.8283 - loss: 0.4463 - val_accuracy: 0.7123 - val_loss: 0.6980 - learning_rate: 1.0000e-06\n",
      "Epoch 71/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.7866 - loss: 0.5357 - val_accuracy: 0.7123 - val_loss: 0.6991 - learning_rate: 1.0000e-06\n",
      "Epoch 72/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.8167 - loss: 0.4621 - val_accuracy: 0.7055 - val_loss: 0.6992 - learning_rate: 1.0000e-06\n",
      "Epoch 73/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - accuracy: 0.8066 - loss: 0.4895 - val_accuracy: 0.7055 - val_loss: 0.6999 - learning_rate: 1.0000e-06\n",
      "Epoch 74/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.8113 - loss: 0.4541 - val_accuracy: 0.7055 - val_loss: 0.6999 - learning_rate: 1.0000e-06\n",
      "Epoch 75/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.8129 - loss: 0.4590 - val_accuracy: 0.7055 - val_loss: 0.7012 - learning_rate: 1.0000e-06\n",
      "Epoch 76/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8253 - loss: 0.4224 - val_accuracy: 0.7055 - val_loss: 0.7023 - learning_rate: 1.0000e-06\n",
      "Epoch 77/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8152 - loss: 0.4524 - val_accuracy: 0.7123 - val_loss: 0.7038 - learning_rate: 1.0000e-06\n",
      "Epoch 78/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.8260 - loss: 0.4904 - val_accuracy: 0.7123 - val_loss: 0.7037 - learning_rate: 1.0000e-06\n",
      "Epoch 79/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.8239 - loss: 0.4087 - val_accuracy: 0.7123 - val_loss: 0.7042 - learning_rate: 1.0000e-06\n",
      "Epoch 80/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.7882 - loss: 0.4896 - val_accuracy: 0.7192 - val_loss: 0.7047 - learning_rate: 1.0000e-06\n",
      "Epoch 81/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8597 - loss: 0.4018 - val_accuracy: 0.7260 - val_loss: 0.7065 - learning_rate: 1.0000e-06\n",
      "Epoch 82/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.8209 - loss: 0.4620 - val_accuracy: 0.7192 - val_loss: 0.7067 - learning_rate: 1.0000e-06\n",
      "Epoch 83/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.7981 - loss: 0.4840 - val_accuracy: 0.7192 - val_loss: 0.7075 - learning_rate: 1.0000e-06\n",
      "Epoch 84/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.7935 - loss: 0.5057 - val_accuracy: 0.7192 - val_loss: 0.7075 - learning_rate: 1.0000e-06\n",
      "Epoch 85/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.8117 - loss: 0.4783 - val_accuracy: 0.7192 - val_loss: 0.7081 - learning_rate: 1.0000e-06\n",
      "Epoch 86/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.7999 - loss: 0.4766 - val_accuracy: 0.7192 - val_loss: 0.7080 - learning_rate: 1.0000e-06\n",
      "Epoch 87/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.8342 - loss: 0.4097 - val_accuracy: 0.7192 - val_loss: 0.7084 - learning_rate: 1.0000e-06\n",
      "Epoch 88/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.8250 - loss: 0.4382 - val_accuracy: 0.7192 - val_loss: 0.7083 - learning_rate: 1.0000e-06\n",
      "Epoch 89/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.8179 - loss: 0.4705 - val_accuracy: 0.7192 - val_loss: 0.7097 - learning_rate: 1.0000e-06\n",
      "Epoch 90/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 76ms/step - accuracy: 0.7953 - loss: 0.4875 - val_accuracy: 0.7192 - val_loss: 0.7091 - learning_rate: 1.0000e-06\n",
      "Epoch 91/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - accuracy: 0.7999 - loss: 0.4734 - val_accuracy: 0.7260 - val_loss: 0.7086 - learning_rate: 1.0000e-06\n",
      "Epoch 92/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.8203 - loss: 0.4263 - val_accuracy: 0.7192 - val_loss: 0.7081 - learning_rate: 1.0000e-06\n",
      "Epoch 93/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8091 - loss: 0.4387 - val_accuracy: 0.7192 - val_loss: 0.7080 - learning_rate: 1.0000e-06\n",
      "Epoch 94/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.8185 - loss: 0.4397 - val_accuracy: 0.7192 - val_loss: 0.7087 - learning_rate: 1.0000e-06\n",
      "Epoch 95/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.8375 - loss: 0.4169 - val_accuracy: 0.7192 - val_loss: 0.7087 - learning_rate: 1.0000e-06\n",
      "Epoch 96/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.7776 - loss: 0.5119 - val_accuracy: 0.7192 - val_loss: 0.7087 - learning_rate: 1.0000e-06\n",
      "Epoch 97/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8038 - loss: 0.4479 - val_accuracy: 0.7192 - val_loss: 0.7090 - learning_rate: 1.0000e-06\n",
      "Epoch 98/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8115 - loss: 0.4750 - val_accuracy: 0.7192 - val_loss: 0.7098 - learning_rate: 1.0000e-06\n",
      "Epoch 99/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.8415 - loss: 0.4177 - val_accuracy: 0.7192 - val_loss: 0.7099 - learning_rate: 1.0000e-06\n",
      "Epoch 100/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.8187 - loss: 0.4635 - val_accuracy: 0.7192 - val_loss: 0.7104 - learning_rate: 1.0000e-06\n",
      "Mean Accuracy: 0.7335\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, LeakyReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Train Word2Vec using skip-gram with increased epochs (50)\n",
    "sentences = train_df['Text'].tolist()\n",
    "w2v_model = Word2Vec(sentences, vector_size=300, window=7, min_count=2, workers=4, sg=1, negative=10, epochs=50)\n",
    "w2v_model.save(\"word2vec.model\")\n",
    "\n",
    "def get_sentence_vector(sentence, model, vector_size=300):\n",
    "    vectors = [model.wv[word] for word in sentence if word in model.wv]\n",
    "    if vectors:\n",
    "        vectors = np.array(vectors)\n",
    "        avg_vec = np.mean(vectors, axis=0)\n",
    "        max_vec = np.max(vectors, axis=0)\n",
    "        min_vec = np.min(vectors, axis=0)\n",
    "        return np.concatenate([avg_vec, max_vec, min_vec])\n",
    "    else:\n",
    "        return np.zeros(vector_size * 3)\n",
    "\n",
    "train_df['vector'] = train_df['Text'].apply(lambda x: get_sentence_vector(x, w2v_model))\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "train_df['Sentiment'] = label_encoder.fit_transform(train_df['Sentiment'])\n",
    "X = np.vstack(train_df['vector'].values)\n",
    "y = train_df['Sentiment'].values.astype(np.int32)\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "accuracies = []\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    model = Sequential([\n",
    "        Dense(2048, input_shape=(900,)),\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        \n",
    "        Dense(1024),\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        \n",
    "        Dense(512),\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        \n",
    "        Dense(256),\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),\n",
    "        \n",
    "        Dense(128),\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),\n",
    "        \n",
    "        Dense(3, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1, min_lr=1e-6)\n",
    "    \n",
    "    model.fit(X_train, y_train, epochs=100, batch_size=64, validation_data=(X_test, y_test), callbacks=[lr_scheduler], verbose=1)\n",
    "    loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "print(f\"Mean Accuracy: {np.mean(accuracies):.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4245661,
     "sourceId": 7316566,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
