{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\ketan\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     C:\\Users\\ketan\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\ketan\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\ketan\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 96,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import defaultdict\n",
        "import ast\n",
        "import ast\n",
        "import re\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import wordnet, stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_path = \"dataset\\TRAIN.csv\"\n",
        "test_path = \"dataset\\TEST.csv\"\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "train_data = pd.read_csv(train_path, header=None, names=['sentence'])\n",
        "test_data = pd.read_csv(test_path, header=None, names=['sentence'])\n",
        "# stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# def preprocess_text(text):\n",
        "#     # Convert string representation of list into actual list\n",
        "#     try:\n",
        "#         words_with_tags = ast.literal_eval(text)\n",
        "#     except (SyntaxError, ValueError):\n",
        "#         return \"\"\n",
        "    \n",
        "#     # Keep words and tags unchanged\n",
        "#     words_with_tags = [(word.lower(), tag) for word, tag in words_with_tags]\n",
        "    \n",
        "#     return words_with_tags\n",
        "\n",
        "# train_data['sentence'] = train_data['sentence'].apply(ast.literal_eval)\n",
        "# test_data['sentence'] = test_data['sentence'].apply(ast.literal_eval)\n",
        "# def preprocess_text(text):\n",
        "#     \"\"\"Convert string representation of list into actual list and lowercase words.\"\"\"\n",
        "#     if not isinstance(text, str):  # Ensure text is a string\n",
        "#         return []\n",
        "    \n",
        "#     try:\n",
        "#         words_with_tags = ast.literal_eval(text)\n",
        "#         if not isinstance(words_with_tags, list):  # Ensure it's a list\n",
        "#             return []\n",
        "#     except (SyntaxError, ValueError):\n",
        "#         return []\n",
        "    \n",
        "#     return [(str(word).lower(), str(tag)) for word, tag in words_with_tags]  # Convert words & tags to string\n",
        "\n",
        "# # Apply preprocessing to dataset\n",
        "# train_data['sentence'] = train_data['sentence'].apply(preprocess_text)\n",
        "# test_data['sentence'] = test_data['sentence'].apply(preprocess_text)\n",
        "\n",
        "# def preprocess_text(text):\n",
        "#     \"\"\"Convert string representation of list into actual list, lowercase words, and lemmatize.\"\"\"\n",
        "#     if not isinstance(text, str):  # Ensure text is a string\n",
        "#         return []\n",
        "    \n",
        "#     try:\n",
        "#         words_with_tags = ast.literal_eval(text)\n",
        "#         if not isinstance(words_with_tags, list):  # Ensure it's a list\n",
        "#             return []\n",
        "#     except (SyntaxError, ValueError):\n",
        "#         return []\n",
        "    \n",
        "#     return [(lemmatizer.lemmatize(str(word).lower()), str(tag)) for word, tag in words_with_tags]  # Convert words & tags to string & lemmatize\n",
        "\n",
        "# # Apply preprocessing to dataset\n",
        "# train_data['sentence'] = train_data['sentence'].apply(preprocess_text)\n",
        "# test_data['sentence'] = test_data['sentence'].apply(preprocess_text)\n",
        "# def get_synonym(word):\n",
        "#     synonyms = wordnet.synsets(word)\n",
        "#     if synonyms:\n",
        "#         return synonyms[0].lemmas()[0].name()  # Return first synonym\n",
        "#     return word\n",
        "\n",
        "# def preprocess_text(text):\n",
        "#     \"\"\"Convert string representation of list into actual list, lowercase words, lemmatize, and replace rare words with synonyms.\"\"\"\n",
        "#     if not isinstance(text, str):  # Ensure text is a string\n",
        "#         return []\n",
        "    \n",
        "#     try:\n",
        "#         words_with_tags = ast.literal_eval(text)\n",
        "#         if not isinstance(words_with_tags, list):  # Ensure it's a list\n",
        "#             return []\n",
        "#     except (SyntaxError, ValueError):\n",
        "#         return []\n",
        "    \n",
        "#     return [(get_synonym(lemmatizer.lemmatize(str(word).lower())), str(tag)) for word, tag in words_with_tags]  # Convert words & tags to string, lemmatize, and apply synonyms\n",
        "\n",
        "# # Apply preprocessing to dataset\n",
        "# train_data['sentence'] = train_data['sentence'].apply(preprocess_text)\n",
        "# test_data['sentence'] = test_data['sentence'].apply(preprocess_text)\n",
        "def get_synonym(word):\n",
        "    synonyms = wordnet.synsets(word)\n",
        "    if synonyms:\n",
        "        return synonyms[0].lemmas()[0].name()  # Return first synonym\n",
        "    return word\n",
        "\n",
        "def augment_text(text):\n",
        "    \"\"\"Create both original and synonym-replaced versions of the sentence.\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return []\n",
        "    \n",
        "    try:\n",
        "        words_with_tags = ast.literal_eval(text)\n",
        "        if not isinstance(words_with_tags, list):\n",
        "            return []\n",
        "    except (SyntaxError, ValueError):\n",
        "        return []\n",
        "    \n",
        "    # Original sentence\n",
        "    original = [(lemmatizer.lemmatize(str(word).lower()), str(tag)) for word, tag in words_with_tags]\n",
        "    \n",
        "    # Augmented sentence with synonyms\n",
        "    augmented = [(get_synonym(word), tag) for word, tag in original]\n",
        "    \n",
        "    return [original, augmented]  # Return both versions\n",
        "\n",
        "# Apply preprocessing to dataset with augmentation\n",
        "augmented_sentences = train_data['sentence'].apply(augment_text)\n",
        "\n",
        "# Flatten the list so both original & augmented sentences are added\n",
        "train_data = pd.DataFrame({'sentence': [sent for pair in augmented_sentences for sent in pair]})\n",
        "\n",
        "test_data['sentence'] = test_data['sentence'].apply(lambda x: [(lemmatizer.lemmatize(str(word).lower()), str(tag)) for word, tag in ast.literal_eval(x)] if isinstance(x, str) else [])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def get_synonym(word):\n",
        "#     synonyms = wordnet.synsets(word)\n",
        "#     if synonyms:\n",
        "#         return synonyms[0].lemmas()[0].name()  # Return first synonym\n",
        "#     return word\n",
        "\n",
        "# def augment_text(text):\n",
        "#     \"\"\"Create both original and synonym-replaced versions of the sentence, while removing stopwords.\"\"\"\n",
        "#     if not isinstance(text, str):\n",
        "#         return []\n",
        "    \n",
        "#     try:\n",
        "#         words_with_tags = ast.literal_eval(text)\n",
        "#         if not isinstance(words_with_tags, list):\n",
        "#             return []\n",
        "#     except (SyntaxError, ValueError):\n",
        "#         return []\n",
        "    \n",
        "#     # Original sentence with stopword removal\n",
        "#     original = [(lemmatizer.lemmatize(str(word).lower()), str(tag)) for word, tag in words_with_tags if word.lower() not in stop_words]\n",
        "    \n",
        "#     # Augmented sentence with synonyms\n",
        "#     augmented = [(get_synonym(word), tag) for word, tag in original]\n",
        "    \n",
        "#     return [original, augmented]  # Return both versions\n",
        "\n",
        "# # Apply preprocessing to dataset with augmentation\n",
        "# augmented_sentences = train_data['sentence'].apply(augment_text)\n",
        "\n",
        "# # Flatten the list so both original & augmented sentences are added\n",
        "# train_data = pd.DataFrame({'sentence': [sent for pair in augmented_sentences for sent in pair]})\n",
        "\n",
        "# test_data['sentence'] = test_data['sentence'].apply(lambda x: [(lemmatizer.lemmatize(str(word).lower()), str(tag)) for word, tag in ast.literal_eval(x) if word.lower() not in stop_words] if isinstance(x, str) else [])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'_', 'NUM', 'ADV', '.', 'PRON', 'CCONJ', 'VERB', 'DET', 'CONJ', 'ADJ', 'ADP', 'PRT', 'PUNCT', 'SYM', 'X', 'PART', 'INTJ', 'AUX', 'NOUN', 'SCONJ', 'PROPN'}\n"
          ]
        }
      ],
      "source": [
        "words = set(word for sentence in train_data['sentence'] for word, _ in sentence)\n",
        "tags = set(tag for sentence in train_data['sentence'] for _, tag in sentence)\n",
        "print(tags)\n",
        "\n",
        "transition_counts = defaultdict(lambda: defaultdict(int))\n",
        "emission_counts = defaultdict(lambda: defaultdict(int))\n",
        "initial_counts = defaultdict(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_hmm(train_data):\n",
        "    for sentence in train_data['sentence']:\n",
        "        prev_tag = None\n",
        "        for word, tag in sentence:\n",
        "            emission_counts[tag][word] += 1\n",
        "            if prev_tag is not None:\n",
        "                transition_counts[prev_tag][tag] += 1\n",
        "            else:\n",
        "                initial_counts[tag] += 1\n",
        "            prev_tag = tag\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [],
      "source": [
        "def normalize_counts(counts):\n",
        "    probabilities = {}\n",
        "    for key, sub_counts in counts.items():\n",
        "        total = sum(sub_counts.values())\n",
        "        probabilities[key] = {k: v / total for k, v in sub_counts.items()}\n",
        "    return probabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_hmm(train_data)\n",
        "transition_probs = normalize_counts(transition_counts)\n",
        "emission_probs = normalize_counts(emission_counts)\n",
        "initial_probs = {k: v / sum(initial_counts.values()) for k, v in initial_counts.items()}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [],
      "source": [
        "def viterbi(words, transition_probs, emission_probs, initial_probs, tags):\n",
        "    V = [{}]\n",
        "    backpointer = [{}]\n",
        "    \n",
        "    for tag in tags:\n",
        "        V[0][tag] = initial_probs.get(tag, 0) * emission_probs[tag].get(words[0], 0)\n",
        "        backpointer[0][tag] = None\n",
        "    \n",
        "    for t in range(1, len(words)):\n",
        "        V.append({})\n",
        "        backpointer.append({})\n",
        "        for tag in tags:\n",
        "            max_prob, best_prev_tag = max(\n",
        "                (V[t-1][prev_tag] * transition_probs[prev_tag].get(tag, 0) * emission_probs[tag].get(words[t], 0), prev_tag)\n",
        "                for prev_tag in tags\n",
        "            )\n",
        "            V[t][tag] = max_prob\n",
        "            backpointer[t][tag] = best_prev_tag\n",
        "    \n",
        "    best_last_tag = max(V[-1], key=V[-1].get)\n",
        "    best_path = [best_last_tag]\n",
        "    for t in range(len(words) - 1, 0, -1):\n",
        "        best_path.insert(0, backpointer[t][best_path[0]])\n",
        "    \n",
        "    return best_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def viterbi(words, transition_probs, emission_probs, initial_probs, tags):\n",
        "#     if not words:  # Check if words list is empty\n",
        "#         return []\n",
        "\n",
        "#     V = [{}]\n",
        "#     backpointer = [{}]\n",
        "\n",
        "#     for tag in tags:\n",
        "#         V[0][tag] = initial_probs.get(tag, 0) * emission_probs[tag].get(words[0], 0)\n",
        "#         backpointer[0][tag] = None\n",
        "\n",
        "#     for t in range(1, len(words)):\n",
        "#         V.append({})\n",
        "#         backpointer.append({})\n",
        "#         for tag in tags:\n",
        "#             max_prob, best_prev_tag = max(\n",
        "#                 (V[t-1][prev_tag] * transition_probs[prev_tag].get(tag, 0) * emission_probs[tag].get(words[t], 0), prev_tag)\n",
        "#                 for prev_tag in tags\n",
        "#             )\n",
        "#             V[t][tag] = max_prob\n",
        "#             backpointer[t][tag] = best_prev_tag\n",
        "\n",
        "#     best_last_tag = max(V[-1], key=V[-1].get)\n",
        "#     best_path = [best_last_tag]\n",
        "#     for t in range(len(words) - 1, 0, -1):\n",
        "#         best_path.insert(0, backpointer[t][best_path[0]])\n",
        "\n",
        "#     return best_path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Accuracy: 83.15894936982683%\n"
          ]
        }
      ],
      "source": [
        "def evaluate(test_data):\n",
        "    correct, total = 0, 0\n",
        "    for sentence in test_data['sentence']:\n",
        "        words = [word for word,_ in sentence]\n",
        "        actual_tags = [tag for _, tag in sentence]\n",
        "        predicted_tags = viterbi(words, transition_probs, emission_probs, initial_probs, tags)\n",
        "        correct += sum(p == a for p, a in zip(predicted_tags, actual_tags))\n",
        "        total += len(actual_tags)\n",
        "    accuracy = correct / total if total > 0 else 0\n",
        "    print(\"Model Accuracy: \"+str(accuracy*100)+\"%\")\n",
        "\n",
        "evaluate(test_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "viterbi() missing 1 required positional argument: 'tags'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m sentence\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHe\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdrew\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdeep\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbreath\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 2\u001b[0m predicted_tags \u001b[38;5;241m=\u001b[39m \u001b[43mviterbi\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransition_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43memission_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_probs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m predicted_tags\n",
            "\u001b[1;31mTypeError\u001b[0m: viterbi() missing 1 required positional argument: 'tags'"
          ]
        }
      ],
      "source": [
        "sentence=['He','drew','a','deep','breath']\n",
        "predicted_tags = viterbi(sentence, transition_probs, emission_probs, initial_probs)\n",
        "predicted_tags"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
